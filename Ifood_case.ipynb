{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/murillo-borges/ifood-ab-campaign-case/blob/main/Ifood_case.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFN2NnqYmCdy"
      },
      "source": [
        "## Prepara√ß√£o do ambiente e ETL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAPEK33LgNwq"
      },
      "source": [
        "### Database Pedidos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh7E-EBFhrMs",
        "outputId": "0d62fed8-e5ab-418a-a630-2a6adcea6d61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚öôÔ∏è Instalando Java e PySpark...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxt-dev libxtst6 libxxf86dga1 openjdk-11-jre\n",
            "  x11-utils\n",
            "Suggested packages:\n",
            "  libxt-doc openjdk-11-demo openjdk-11-source visualvm mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxt-dev libxtst6 libxxf86dga1 openjdk-11-jdk\n",
            "  openjdk-11-jre x11-utils\n",
            "0 upgraded, 10 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 6,920 kB of archives.\n",
            "After this operation, 16.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.27+6~us1-0ubuntu1~22.04 [214 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk amd64 11.0.27+6~us1-0ubuntu1~22.04 [2,895 kB]\n",
            "Fetched 6,920 kB in 1s (6,549 kB/s)\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../0-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../1-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../2-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../3-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../4-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../5-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../6-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../7-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../8-openjdk-11-jre_11.0.27+6~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.27+6~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-11-jdk:amd64.\n",
            "Preparing to unpack .../9-openjdk-11-jdk_11.0.27+6~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jdk:amd64 (11.0.27+6~us1-0ubuntu1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.27+6~us1-0ubuntu1~22.04) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up openjdk-11-jdk:amd64 (11.0.27+6~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "‚úÖ Ambiente configurado!\n",
            "\n",
            "üöÄ Iniciando sess√£o Spark...\n",
            "‚úÖ SparkSession criada!\n",
            "\n",
            "üì• Baixando o arquivo order.json.gz...\n",
            "‚úÖ Download conclu√≠do!\n",
            "\n",
            "üìä Lendo arquivo JSON com PySpark...\n",
            "‚úÖ Dados carregados!\n",
            "\n",
            "üîç Primeiras linhas do DataFrame:\n",
            "+-----------+----------------------------------------------------------------+-------------+---------------------+------------------------+-------------------------+----------------------------+-------------------------+--------------------------+----------------------+-------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------+-----------------+------------------+-----------------+------------------------+----------------------------------------------------------------+---------------+--------------------+------------------+---------------+\n",
            "|cpf        |customer_id                                                     |customer_name|delivery_address_city|delivery_address_country|delivery_address_district|delivery_address_external_id|delivery_address_latitude|delivery_address_longitude|delivery_address_state|delivery_address_zip_code|items                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |merchant_id                                                     |merchant_latitude|merchant_longitude|merchant_timezone|order_created_at        |order_id                                                        |order_scheduled|order_scheduled_date|order_total_amount|origin_platform|\n",
            "+-----------+----------------------------------------------------------------+-------------+---------------------+------------------------+-------------------------+----------------------------+-------------------------+--------------------------+----------------------+-------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------+-----------------+------------------+-----------------+------------------------+----------------------------------------------------------------+---------------+--------------------+------------------+---------------+\n",
            "|80532101763|7ba88a68bb2a3504c6bd37a707af57a0b8d6e110a551c719c9e7f71aaf3a99cf|GUSTAVO      |FRANCA               |BR                      |JARDIM ESPRAIADO         |6736655                     |-47.39                   |-20.55                    |SP                    |14403                    |[{\"name\": \"Parmegiana de Fil√© de Frango (2 pessoas)\", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 1, \"unitPrice\": {\"value\": \"2800\", \"currency\": \"BRL\"}, \"externalId\": \"0bcd6764fd5e466d9c04b18ac0eb69e6\", \"totalValue\": {\"value\": \"2800\", \"currency\": \"BRL\"}, \"customerNote\": null, \"garnishItems\": [{\"name\": \"COM Arroz branco\", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 2, \"unitPrice\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"categoryId\": \"13HDH\", \"externalId\": \"384bd2b4eb7d454d8e0274e7d590ab4f\", \"totalValue\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"categoryName\": \"PERSONALIZAR\", \"integrationId\": null}], \"integrationId\": \"PMFR\", \"totalAddition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"totalDiscount\": {\"value\": \"0\", \"currency\": \"BRL\"}}, {\"name\": \"Lasanha Frango (2 pessoas)\", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 3, \"unitPrice\": {\"value\": \"1800\", \"currency\": \"BRL\"}, \"externalId\": \"a361f5eec6a44ac0817892e81bf22e80\", \"totalValue\": {\"value\": \"1800\", \"currency\": \"BRL\"}, \"customerNote\": null, \"garnishItems\": [], \"integrationId\": \"LF\", \"totalAddition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"totalDiscount\": {\"value\": \"0\", \"currency\": \"BRL\"}}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |a992a079a651e699d9149423761df2427c0e3af0a2a1b5bb6d2bad1cb3a3a265|-47.39           |-20.55            |America/Sao_Paulo|2019-01-17T22:50:06.000Z|33e0612d62e5eb42aba15b58413137e441fbe906de2febd6a6c721b0e5773b44|false          |NULL                |46.0              |ANDROID        |\n",
            "|43352103961|078acecdcf7fa89d356bfa349f14a8219db1ee161ce28ae4d9fee550c95d6fbd|MICHELLE     |SANTOS               |BR                      |CAMPO GRANDE             |8759216                     |-46.34                   |-23.96                    |SP                    |11070                    |[{\"name\": \"Fil√© Mignon √† Cubana\", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 1, \"unitPrice\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"externalId\": \"e0e81b2027c241cca5bf60a4768800ec\", \"totalValue\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"customerNote\": null, \"garnishItems\": [{\"name\": \"334 - 1/2 por√ß√£o \", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 2, \"unitPrice\": {\"value\": \"7350\", \"currency\": \"BRL\"}, \"categoryId\": \"1J3T0\", \"externalId\": \"b869c10747d1417d8968d7e3e1bd7de4\", \"totalValue\": {\"value\": \"7350\", \"currency\": \"BRL\"}, \"categoryName\": \"escolha sua por√ß√£o \", \"integrationId\": null}], \"integrationId\": null, \"totalAddition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"totalDiscount\": {\"value\": \"0\", \"currency\": \"BRL\"}}, {\"name\": \"603- Pudim de leite\", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 4, \"unitPrice\": {\"value\": \"800\", \"currency\": \"BRL\"}, \"externalId\": \"4436f76edc164052aaf3c21a6567b570\", \"totalValue\": {\"value\": \"800\", \"currency\": \"BRL\"}, \"customerNote\": null, \"garnishItems\": [], \"integrationId\": null, \"totalAddition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"totalDiscount\": {\"value\": \"0\", \"currency\": \"BRL\"}}, {\"name\": \"601-Torta Holandesa\", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 5, \"unitPrice\": {\"value\": \"800\", \"currency\": \"BRL\"}, \"externalId\": \"c0b788e876c54f198307dc142fed7aff\", \"totalValue\": {\"value\": \"800\", \"currency\": \"BRL\"}, \"customerNote\": null, \"garnishItems\": [], \"integrationId\": null, \"totalAddition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"totalDiscount\": {\"value\": \"0\", \"currency\": \"BRL\"}}, {\"name\": \"513 - Guarni√ß√£o Completa II\", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 3, \"unitPrice\": {\"value\": \"1500\", \"currency\": \"BRL\"}, \"externalId\": \"385fb04a358340878b14116df9970e46\", \"totalValue\": {\"value\": \"1500\", \"currency\": \"BRL\"}, \"customerNote\": null, \"garnishItems\": [], \"integrationId\": null, \"totalAddition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"totalDiscount\": {\"value\": \"0\", \"currency\": \"BRL\"}}]                                                                                                                                                                                                           |5152f28ee0518b8803ccf0a4096eb2ff8b81e9491861c9d1b673846ae027b8b0|-46.34           |-23.96            |America/Sao_Paulo|2019-01-17T17:51:26.000Z|148c4353a2952f3fe7973547283265eb22b575fb712ed2618a6cf2d813b987ad|false          |NULL                |104.5             |ANDROID        |\n",
            "|38650991217|0e38a3237b5946e8ab2367b4f1a3ae6e77f1e215bc760ca332153516c13b9c2e|VICTOR       |GUARULHOS            |BR                      |JARDIM ROSSI             |8765930                     |-46.53                   |-23.44                    |SP                    |71304                    |[{\"name\": \"GRANDE 2 SABORES\", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 1, \"unitPrice\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"externalId\": \"87a24efdd88c4b7b8bf1132cabfd4195\", \"totalValue\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"customerNote\": null, \"garnishItems\": [{\"name\": \"TRADICIONAL\", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 2, \"unitPrice\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"categoryId\": \"0025\", \"externalId\": \"2b89a4d4d6f14541a8830e5808d1f09f\", \"totalValue\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"categoryName\": \"Escolha a sua Prefer√™ncia\", \"integrationId\": null}, {\"name\": \"1/2 21 - PIZZA FRANGO COM BACON E CATUPIRY \", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 3, \"unitPrice\": {\"value\": \"1750\", \"currency\": \"BRL\"}, \"categoryId\": \"SABOR\", \"externalId\": \"c7f63cc58a3643c7aa3f24858c7029f4\", \"totalValue\": {\"value\": \"1750\", \"currency\": \"BRL\"}, \"categoryName\": \"Escolha um sabor\", \"integrationId\": null}, {\"name\": \"1/2 21 - PIZZA FRANGO COM BACON E CATUPIRY \", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 3, \"unitPrice\": {\"value\": \"1750\", \"currency\": \"BRL\"}, \"categoryId\": \"SABOR2\", \"externalId\": \"c7f63cc58a3643c7aa3f24858c7029f4\", \"totalValue\": {\"value\": \"1750\", \"currency\": \"BRL\"}, \"categoryName\": \"Escolha o segundo sabor\", \"integrationId\": null}, {\"name\": \"1/2 29 - PIZZA √Ä MODA DA CASA\", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 4, \"unitPrice\": {\"value\": \"1750\", \"currency\": \"BRL\"}, \"categoryId\": \"SABOR\", \"externalId\": \"0dcd4a195d6243ec806eb5a29682c797\", \"totalValue\": {\"value\": \"1750\", \"currency\": \"BRL\"}, \"categoryName\": \"Escolha um sabor\", \"integrationId\": null}, {\"name\": \"1/2 29 - PIZZA √Ä MODA DA CASA\", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 4, \"unitPrice\": {\"value\": \"1750\", \"currency\": \"BRL\"}, \"categoryId\": \"SABOR2\", \"externalId\": \"0dcd4a195d6243ec806eb5a29682c797\", \"totalValue\": {\"value\": \"1750\", \"currency\": \"BRL\"}, \"categoryName\": \"Escolha o segundo sabor\", \"integrationId\": null}], \"integrationId\": null, \"totalAddition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"totalDiscount\": {\"value\": \"0\", \"currency\": \"BRL\"}}]|b6096419455c35d06105a5ef0d25c51f9dd40e1e99ac3353c133a9bae4e5ddad|-46.53           |-23.44            |America/Sao_Paulo|2019-01-17T22:53:47.000Z|c37e495a91b498bb7b70a9e09ac115d0cdd443f152dc112a5476745f84ca16cd|false          |NULL                |35.0              |IOS            |\n",
            "|63579726866|cab1a004b7206d07910092c515a79834fea0a03d7d905483121d8450003eb24c|ANNIE        |SAO PAULO            |BR                      |PARQUE SAO JORGE         |7834087                     |-46.57                   |-23.53                    |SP                    |30870                    |[{\"name\": \"CALABRESA\", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 1, \"unitPrice\": {\"value\": \"2040\", \"currency\": \"BRL\"}, \"externalId\": \"5b598957c69a4aa68e02a20e3ce617a1\", \"totalValue\": {\"value\": \"2040\", \"currency\": \"BRL\"}, \"customerNote\": null, \"garnishItems\": [], \"integrationId\": null, \"totalAddition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"totalDiscount\": {\"value\": \"0\", \"currency\": \"BRL\"}}, {\"name\": \"CHEESE SALADA\", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 2, \"unitPrice\": {\"value\": \"2040\", \"currency\": \"BRL\"}, \"externalId\": \"3a64b961d342467ea23792556c99f7c2\", \"totalValue\": {\"value\": \"2040\", \"currency\": \"BRL\"}, \"customerNote\": \"Sem tomate\", \"garnishItems\": [], \"integrationId\": null, \"totalAddition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"totalDiscount\": {\"value\": \"0\", \"currency\": \"BRL\"}}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |082bfdcdf6ccdc343e3c4d25ee376b5b6ca7e96ad8b04e66db1391f3fd0d34c9|-46.57           |-23.53            |America/Sao_Paulo|2019-01-17T23:56:53.000Z|b4df94142d21354611247da9ca94f870c09b93989b531afdcc251e06fc90af03|false          |NULL                |40.8              |IOS            |\n",
            "|90617788806|aa7edf5b166b8c843aec3b96dc561222888734f3879123e89e65521faff96e9a|DANIEL       |VITORIA              |BR                      |JARDIM CAMBURI           |7211683                     |-40.27                   |-20.25                    |ES                    |29090                    |[{\"name\": \"GRANDE (35CM) 8 PD√á 2 SABORES\", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 1, \"unitPrice\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"externalId\": \"83127d50665f4a269febd427eb49306b\", \"totalValue\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"customerNote\": null, \"garnishItems\": [{\"name\": \"BORDA CATUPIRY GR√ÅTIS \", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 2, \"unitPrice\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"categoryId\": \"0025\", \"externalId\": \"e9a71c078a6045eabb09bd4d38b991c6\", \"totalValue\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"categoryName\": \"Escolha a sua Prefer√™ncia\", \"integrationId\": null}, {\"name\": \"1/2 MISTA\", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 3, \"unitPrice\": {\"value\": \"2425\", \"currency\": \"BRL\"}, \"categoryId\": \"SABOR\", \"externalId\": \"46510932dfd448af910ae16e112a8d3a\", \"totalValue\": {\"value\": \"2425\", \"currency\": \"BRL\"}, \"categoryName\": \"Escolha um sabor\", \"integrationId\": null}, {\"name\": \"1/2 MISTA\", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 3, \"unitPrice\": {\"value\": \"2425\", \"currency\": \"BRL\"}, \"categoryId\": \"SABOR2\", \"externalId\": \"46510932dfd448af910ae16e112a8d3a\", \"totalValue\": {\"value\": \"2425\", \"currency\": \"BRL\"}, \"categoryName\": \"Escolha o segundo sabor\", \"integrationId\": null}, {\"name\": \"1/2 √Ä MODA CAPIXABA\", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 4, \"unitPrice\": {\"value\": \"2425\", \"currency\": \"BRL\"}, \"categoryId\": \"SABOR\", \"externalId\": \"5be89603526f4c42a6d9ac9e05f8a19a\", \"totalValue\": {\"value\": \"2425\", \"currency\": \"BRL\"}, \"categoryName\": \"Escolha um sabor\", \"integrationId\": null}, {\"name\": \"1/2 √Ä MODA CAPIXABA\", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 4, \"unitPrice\": {\"value\": \"2425\", \"currency\": \"BRL\"}, \"categoryId\": \"SABOR2\", \"externalId\": \"5be89603526f4c42a6d9ac9e05f8a19a\", \"totalValue\": {\"value\": \"2425\", \"currency\": \"BRL\"}, \"categoryName\": \"Escolha o segundo sabor\", \"integrationId\": null}], \"integrationId\": null, \"totalAddition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"totalDiscount\": {\"value\": \"0\", \"currency\": \"BRL\"}}]                                                                |d7adb764bac29ccb77fb8f746ffbd531bf05ec30a7e1306afed1fc1a821b7854|-40.27           |-20.25            |America/Sao_Paulo|2019-01-17T23:40:53.000Z|4ff64b33b272c1886df21b63272220af6a82d1667dba70dad201810d98608dd8|false          |NULL                |48.5              |ANDROID        |\n",
            "+-----------+----------------------------------------------------------------+-------------+---------------------+------------------------+-------------------------+----------------------------+-------------------------+--------------------------+----------------------+-------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------+-----------------+------------------+-----------------+------------------------+----------------------------------------------------------------+---------------+--------------------+------------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "üß† Estrutura do DataFrame:\n",
            "root\n",
            " |-- cpf: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- customer_name: string (nullable = true)\n",
            " |-- delivery_address_city: string (nullable = true)\n",
            " |-- delivery_address_country: string (nullable = true)\n",
            " |-- delivery_address_district: string (nullable = true)\n",
            " |-- delivery_address_external_id: string (nullable = true)\n",
            " |-- delivery_address_latitude: string (nullable = true)\n",
            " |-- delivery_address_longitude: string (nullable = true)\n",
            " |-- delivery_address_state: string (nullable = true)\n",
            " |-- delivery_address_zip_code: string (nullable = true)\n",
            " |-- items: string (nullable = true)\n",
            " |-- merchant_id: string (nullable = true)\n",
            " |-- merchant_latitude: string (nullable = true)\n",
            " |-- merchant_longitude: string (nullable = true)\n",
            " |-- merchant_timezone: string (nullable = true)\n",
            " |-- order_created_at: string (nullable = true)\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- order_scheduled: boolean (nullable = true)\n",
            " |-- order_scheduled_date: string (nullable = true)\n",
            " |-- order_total_amount: double (nullable = true)\n",
            " |-- origin_platform: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1 - Instala√ß√£o e configura√ß√£o do ambiente\n",
        "print(\"‚öôÔ∏è Instalando Java e PySpark...\")\n",
        "!apt-get install openjdk-11-jdk -y\n",
        "!pip install pyspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "print(\"‚úÖ Ambiente configurado!\\n\")\n",
        "\n",
        "# 2 - Iniciando a SparkSession\n",
        "print(\"üöÄ Iniciando sess√£o Spark...\")\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"iFood Order Case\").getOrCreate()\n",
        "print(\"‚úÖ SparkSession criada!\\n\")\n",
        "\n",
        "# 3 - Baixando o arquivo order.json.gz\n",
        "print(\"üì• Baixando o arquivo order.json.gz...\")\n",
        "import urllib.request\n",
        "url = \"https://data-architect-test-source.s3-sa-east-1.amazonaws.com/order.json.gz\"\n",
        "local_path = \"order.json.gz\"\n",
        "urllib.request.urlretrieve(url, local_path)\n",
        "print(\"‚úÖ Download conclu√≠do!\\n\")\n",
        "\n",
        "# 4 - Lendo o arquivo JSON compactado com PySpark\n",
        "print(\"üìä Lendo arquivo JSON com PySpark...\")\n",
        "df_order = spark.read.json(local_path, multiLine=False)\n",
        "print(\"‚úÖ Dados carregados!\\n\")\n",
        "\n",
        "# 5 - Exibindo as primeiras linhas do DataFrame\n",
        "print(\"üîç Primeiras linhas do DataFrame:\")\n",
        "df_order.show(5, truncate=False)\n",
        "\n",
        "# 6 - Exibindo o schema do DataFrame\n",
        "print(\"\\nüß† Estrutura do DataFrame:\")\n",
        "df_order.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b38f66a9",
        "outputId": "61b42f4a-3f87-4cb1-da22-7f40aa68cb1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß† Estrutura do DataFrame com itens explodidos:\n",
            "root\n",
            " |-- cpf: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- customer_name: string (nullable = true)\n",
            " |-- delivery_address_city: string (nullable = true)\n",
            " |-- delivery_address_country: string (nullable = true)\n",
            " |-- delivery_address_district: string (nullable = true)\n",
            " |-- delivery_address_external_id: string (nullable = true)\n",
            " |-- delivery_address_latitude: string (nullable = true)\n",
            " |-- delivery_address_longitude: string (nullable = true)\n",
            " |-- delivery_address_state: string (nullable = true)\n",
            " |-- delivery_address_zip_code: string (nullable = true)\n",
            " |-- items: string (nullable = true)\n",
            " |-- merchant_id: string (nullable = true)\n",
            " |-- merchant_latitude: string (nullable = true)\n",
            " |-- merchant_longitude: string (nullable = true)\n",
            " |-- merchant_timezone: string (nullable = true)\n",
            " |-- order_created_at: string (nullable = true)\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- order_scheduled: boolean (nullable = true)\n",
            " |-- order_scheduled_date: string (nullable = true)\n",
            " |-- order_total_amount: double (nullable = true)\n",
            " |-- origin_platform: string (nullable = true)\n",
            " |-- parsed_items: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- name: string (nullable = true)\n",
            " |    |    |-- addition: struct (nullable = true)\n",
            " |    |    |    |-- value: string (nullable = true)\n",
            " |    |    |    |-- currency: string (nullable = true)\n",
            " |    |    |-- discount: struct (nullable = true)\n",
            " |    |    |    |-- value: string (nullable = true)\n",
            " |    |    |    |-- currency: string (nullable = true)\n",
            " |    |    |-- quantity: double (nullable = true)\n",
            " |    |    |-- sequence: long (nullable = true)\n",
            " |    |    |-- unitPrice: struct (nullable = true)\n",
            " |    |    |    |-- value: string (nullable = true)\n",
            " |    |    |    |-- currency: string (nullable = true)\n",
            " |    |    |-- externalId: string (nullable = true)\n",
            " |    |    |-- totalValue: struct (nullable = true)\n",
            " |    |    |    |-- value: string (nullable = true)\n",
            " |    |    |    |-- currency: string (nullable = true)\n",
            " |    |    |-- customerNote: string (nullable = true)\n",
            " |    |    |-- garnishItems: array (nullable = true)\n",
            " |    |    |    |-- element: struct (containsNull = true)\n",
            " |    |    |    |    |-- name: string (nullable = true)\n",
            " |    |    |    |    |-- addition: struct (nullable = true)\n",
            " |    |    |    |    |    |-- value: string (nullable = true)\n",
            " |    |    |    |    |    |-- currency: string (nullable = true)\n",
            " |    |    |    |    |-- discount: struct (nullable = true)\n",
            " |    |    |    |    |    |-- value: string (nullable = true)\n",
            " |    |    |    |    |    |-- currency: string (nullable = true)\n",
            " |    |    |    |    |-- quantity: double (nullable = true)\n",
            " |    |    |    |    |-- sequence: long (nullable = true)\n",
            " |    |    |    |    |-- unitPrice: struct (nullable = true)\n",
            " |    |    |    |    |    |-- value: string (nullable = true)\n",
            " |    |    |    |    |    |-- currency: string (nullable = true)\n",
            " |    |    |    |    |-- categoryId: string (nullable = true)\n",
            " |    |    |    |    |-- externalId: string (nullable = true)\n",
            " |    |    |    |    |-- totalValue: struct (nullable = true)\n",
            " |    |    |    |    |    |-- value: string (nullable = true)\n",
            " |    |    |    |    |    |-- currency: string (nullable = true)\n",
            " |    |    |    |    |-- categoryName: string (nullable = true)\n",
            " |    |    |    |    |-- integrationId: string (nullable = true)\n",
            " |    |    |-- integrationId: string (nullable = true)\n",
            " |    |    |-- totalAddition: struct (nullable = true)\n",
            " |    |    |    |-- value: string (nullable = true)\n",
            " |    |    |    |-- currency: string (nullable = true)\n",
            " |    |    |-- totalDiscount: struct (nullable = true)\n",
            " |    |    |    |-- value: string (nullable = true)\n",
            " |    |    |    |-- currency: string (nullable = true)\n",
            " |-- item: struct (nullable = true)\n",
            " |    |-- name: string (nullable = true)\n",
            " |    |-- addition: struct (nullable = true)\n",
            " |    |    |-- value: string (nullable = true)\n",
            " |    |    |-- currency: string (nullable = true)\n",
            " |    |-- discount: struct (nullable = true)\n",
            " |    |    |-- value: string (nullable = true)\n",
            " |    |    |-- currency: string (nullable = true)\n",
            " |    |-- quantity: double (nullable = true)\n",
            " |    |-- sequence: long (nullable = true)\n",
            " |    |-- unitPrice: struct (nullable = true)\n",
            " |    |    |-- value: string (nullable = true)\n",
            " |    |    |-- currency: string (nullable = true)\n",
            " |    |-- externalId: string (nullable = true)\n",
            " |    |-- totalValue: struct (nullable = true)\n",
            " |    |    |-- value: string (nullable = true)\n",
            " |    |    |-- currency: string (nullable = true)\n",
            " |    |-- customerNote: string (nullable = true)\n",
            " |    |-- garnishItems: array (nullable = true)\n",
            " |    |    |-- element: struct (containsNull = true)\n",
            " |    |    |    |-- name: string (nullable = true)\n",
            " |    |    |    |-- addition: struct (nullable = true)\n",
            " |    |    |    |    |-- value: string (nullable = true)\n",
            " |    |    |    |    |-- currency: string (nullable = true)\n",
            " |    |    |    |-- discount: struct (nullable = true)\n",
            " |    |    |    |    |-- value: string (nullable = true)\n",
            " |    |    |    |    |-- currency: string (nullable = true)\n",
            " |    |    |    |-- quantity: double (nullable = true)\n",
            " |    |    |    |-- sequence: long (nullable = true)\n",
            " |    |    |    |-- unitPrice: struct (nullable = true)\n",
            " |    |    |    |    |-- value: string (nullable = true)\n",
            " |    |    |    |    |-- currency: string (nullable = true)\n",
            " |    |    |    |-- categoryId: string (nullable = true)\n",
            " |    |    |    |-- externalId: string (nullable = true)\n",
            " |    |    |    |-- totalValue: struct (nullable = true)\n",
            " |    |    |    |    |-- value: string (nullable = true)\n",
            " |    |    |    |    |-- currency: string (nullable = true)\n",
            " |    |    |    |-- categoryName: string (nullable = true)\n",
            " |    |    |    |-- integrationId: string (nullable = true)\n",
            " |    |-- integrationId: string (nullable = true)\n",
            " |    |-- totalAddition: struct (nullable = true)\n",
            " |    |    |-- value: string (nullable = true)\n",
            " |    |    |-- currency: string (nullable = true)\n",
            " |    |-- totalDiscount: struct (nullable = true)\n",
            " |    |    |-- value: string (nullable = true)\n",
            " |    |    |-- currency: string (nullable = true)\n",
            "\n",
            "\n",
            "üîç Primeiras linhas do DataFrame com itens explodidos:\n",
            "+----------------------------------------------------------------+----------------------------------------+--------+--------+--------+--------+-----------+--------------------------------+-----------+------------+-----------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------------+-------------+\n",
            "|order_id                                                        |name                                    |addition|discount|quantity|sequence|unitPrice  |externalId                      |totalValue |customerNote|garnishItems                                                                                                                                   |integrationId|totalAddition|totalDiscount|\n",
            "+----------------------------------------------------------------+----------------------------------------+--------+--------+--------+--------+-----------+--------------------------------+-----------+------------+-----------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------------+-------------+\n",
            "|33e0612d62e5eb42aba15b58413137e441fbe906de2febd6a6c721b0e5773b44|Parmegiana de Fil√© de Frango (2 pessoas)|{0, BRL}|{0, BRL}|1.0     |1       |{2800, BRL}|0bcd6764fd5e466d9c04b18ac0eb69e6|{2800, BRL}|NULL        |[{COM Arroz branco, {0, BRL}, {0, BRL}, 1.0, 2, {0, BRL}, 13HDH, 384bd2b4eb7d454d8e0274e7d590ab4f, {0, BRL}, PERSONALIZAR, NULL}]              |PMFR         |{0, BRL}     |{0, BRL}     |\n",
            "|33e0612d62e5eb42aba15b58413137e441fbe906de2febd6a6c721b0e5773b44|Lasanha Frango (2 pessoas)              |{0, BRL}|{0, BRL}|1.0     |3       |{1800, BRL}|a361f5eec6a44ac0817892e81bf22e80|{1800, BRL}|NULL        |[]                                                                                                                                             |LF           |{0, BRL}     |{0, BRL}     |\n",
            "|148c4353a2952f3fe7973547283265eb22b575fb712ed2618a6cf2d813b987ad|Fil√© Mignon √† Cubana                    |{0, BRL}|{0, BRL}|1.0     |1       |{0, BRL}   |e0e81b2027c241cca5bf60a4768800ec|{0, BRL}   |NULL        |[{334 - 1/2 por√ß√£o , {0, BRL}, {0, BRL}, 1.0, 2, {7350, BRL}, 1J3T0, b869c10747d1417d8968d7e3e1bd7de4, {7350, BRL}, escolha sua por√ß√£o , NULL}]|NULL         |{0, BRL}     |{0, BRL}     |\n",
            "|148c4353a2952f3fe7973547283265eb22b575fb712ed2618a6cf2d813b987ad|603- Pudim de leite                     |{0, BRL}|{0, BRL}|1.0     |4       |{800, BRL} |4436f76edc164052aaf3c21a6567b570|{800, BRL} |NULL        |[]                                                                                                                                             |NULL         |{0, BRL}     |{0, BRL}     |\n",
            "|148c4353a2952f3fe7973547283265eb22b575fb712ed2618a6cf2d813b987ad|601-Torta Holandesa                     |{0, BRL}|{0, BRL}|1.0     |5       |{800, BRL} |c0b788e876c54f198307dc142fed7aff|{800, BRL} |NULL        |[]                                                                                                                                             |NULL         |{0, BRL}     |{0, BRL}     |\n",
            "+----------------------------------------------------------------+----------------------------------------+--------+--------+--------+--------+-----------+--------------------------------+-----------+------------+-----------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import explode, col, from_json\n",
        "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, DoubleType, LongType\n",
        "\n",
        "# Define the schema for the 'items' column\n",
        "items_schema = ArrayType(StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"addition\", StructType([\n",
        "        StructField(\"value\", StringType(), True),\n",
        "        StructField(\"currency\", StringType(), True)\n",
        "    ]), True),\n",
        "    StructField(\"discount\", StructType([\n",
        "        StructField(\"value\", StringType(), True),\n",
        "        StructField(\"currency\", StringType(), True)\n",
        "    ]), True),\n",
        "    StructField(\"quantity\", DoubleType(), True),\n",
        "    StructField(\"sequence\", LongType(), True), # Changed to LongType based on data example\n",
        "    StructField(\"unitPrice\", StructType([\n",
        "        StructField(\"value\", StringType(), True),\n",
        "        StructField(\"currency\", StringType(), True)\n",
        "    ]), True),\n",
        "    StructField(\"externalId\", StringType(), True),\n",
        "    StructField(\"totalValue\", StructType([\n",
        "        StructField(\"value\", StringType(), True),\n",
        "        StructField(\"currency\", StringType(), True)\n",
        "    ]), True),\n",
        "    StructField(\"customerNote\", StringType(), True),\n",
        "    StructField(\"garnishItems\", ArrayType(StructType([\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"addition\", StructType([\n",
        "            StructField(\"value\", StringType(), True),\n",
        "            StructField(\"currency\", StringType(), True)\n",
        "        ]), True),\n",
        "        StructField(\"discount\", StructType([\n",
        "            StructField(\"value\", StringType(), True),\n",
        "            StructField(\"currency\", StringType(), True)\n",
        "        ]), True),\n",
        "        StructField(\"quantity\", DoubleType(), True),\n",
        "        StructField(\"sequence\", LongType(), True), # Changed to LongType based on data example\n",
        "        StructField(\"unitPrice\", StructType([\n",
        "            StructField(\"value\", StringType(), True),\n",
        "            StructField(\"currency\", StringType(), True)\n",
        "        ]), True),\n",
        "        StructField(\"categoryId\", StringType(), True),\n",
        "        StructField(\"externalId\", StringType(), True),\n",
        "        StructField(\"totalValue\", StructType([\n",
        "            StructField(\"value\", StringType(), True),\n",
        "            StructField(\"currency\", StringType(), True)\n",
        "        ]), True),\n",
        "        StructField(\"categoryName\", StringType(), True),\n",
        "        StructField(\"integrationId\", StringType(), True)\n",
        "    ]), True), True),\n",
        "    StructField(\"integrationId\", StringType(), True),\n",
        "    StructField(\"totalAddition\", StructType([\n",
        "        StructField(\"value\", StringType(), True),\n",
        "        StructField(\"currency\", StringType(), True)\n",
        "    ]), True),\n",
        "    StructField(\"totalDiscount\", StructType([\n",
        "        StructField(\"value\", StringType(), True),\n",
        "        StructField(\"currency\", StringType(), True)\n",
        "    ]), True)\n",
        "]))\n",
        "\n",
        "# Parse the JSON string in 'items' column and then explode it\n",
        "df_exploded = df_order.withColumn(\"parsed_items\", from_json(col(\"items\"), items_schema)) \\\n",
        "                      .withColumn(\"item\", explode(\"parsed_items\"))\n",
        "\n",
        "# Show the schema of the new DataFrame with exploded items\n",
        "print(\"üß† Estrutura do DataFrame com itens explodidos:\")\n",
        "df_exploded.printSchema()\n",
        "\n",
        "# Show the first few rows of the new DataFrame\n",
        "print(\"\\nüîç Primeiras linhas do DataFrame com itens explodidos:\")\n",
        "df_exploded.select(\"order_id\", \"item.*\").show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5Zw2QBmrkyz",
        "outputId": "2379b707-764f-462e-da63-c927bf60a9ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+----------------------------------------------------------------+-------------+---------------------+------------------------+-------------------------+----------------------------+-------------------------+--------------------------+----------------------+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------+-----------------+------------------+-----------------+------------------------+----------------------------------------------------------------+---------------+--------------------+------------------+---------------+\n",
            "|cpf        |customer_id                                                     |customer_name|delivery_address_city|delivery_address_country|delivery_address_district|delivery_address_external_id|delivery_address_latitude|delivery_address_longitude|delivery_address_state|delivery_address_zip_code|items                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |merchant_id                                                     |merchant_latitude|merchant_longitude|merchant_timezone|order_created_at        |order_id                                                        |order_scheduled|order_scheduled_date|order_total_amount|origin_platform|\n",
            "+-----------+----------------------------------------------------------------+-------------+---------------------+------------------------+-------------------------+----------------------------+-------------------------+--------------------------+----------------------+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------+-----------------+------------------+-----------------+------------------------+----------------------------------------------------------------+---------------+--------------------+------------------+---------------+\n",
            "|80532101763|7ba88a68bb2a3504c6bd37a707af57a0b8d6e110a551c719c9e7f71aaf3a99cf|GUSTAVO      |FRANCA               |BR                      |JARDIM ESPRAIADO         |6736655                     |-47.39                   |-20.55                    |SP                    |14403                    |[{\"name\": \"Parmegiana de Fil√© de Frango (2 pessoas)\", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 1, \"unitPrice\": {\"value\": \"2800\", \"currency\": \"BRL\"}, \"externalId\": \"0bcd6764fd5e466d9c04b18ac0eb69e6\", \"totalValue\": {\"value\": \"2800\", \"currency\": \"BRL\"}, \"customerNote\": null, \"garnishItems\": [{\"name\": \"COM Arroz branco\", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 2, \"unitPrice\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"categoryId\": \"13HDH\", \"externalId\": \"384bd2b4eb7d454d8e0274e7d590ab4f\", \"totalValue\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"categoryName\": \"PERSONALIZAR\", \"integrationId\": null}], \"integrationId\": \"PMFR\", \"totalAddition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"totalDiscount\": {\"value\": \"0\", \"currency\": \"BRL\"}}, {\"name\": \"Lasanha Frango (2 pessoas)\", \"addition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"discount\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"quantity\": 1.00, \"sequence\": 3, \"unitPrice\": {\"value\": \"1800\", \"currency\": \"BRL\"}, \"externalId\": \"a361f5eec6a44ac0817892e81bf22e80\", \"totalValue\": {\"value\": \"1800\", \"currency\": \"BRL\"}, \"customerNote\": null, \"garnishItems\": [], \"integrationId\": \"LF\", \"totalAddition\": {\"value\": \"0\", \"currency\": \"BRL\"}, \"totalDiscount\": {\"value\": \"0\", \"currency\": \"BRL\"}}]|a992a079a651e699d9149423761df2427c0e3af0a2a1b5bb6d2bad1cb3a3a265|-47.39           |-20.55            |America/Sao_Paulo|2019-01-17T22:50:06.000Z|33e0612d62e5eb42aba15b58413137e441fbe906de2febd6a6c721b0e5773b44|false          |NULL                |46.0              |ANDROID        |\n",
            "+-----------+----------------------------------------------------------------+-------------+---------------------+------------------------+-------------------------+----------------------------+-------------------------+--------------------------+----------------------+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------+-----------------+------------------+-----------------+------------------------+----------------------------------------------------------------+---------------+--------------------+------------------+---------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_order.show(1, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-UndS7-swaM",
        "outputId": "9a1f50b9-37f6-483f-ea19-670eb114a9db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------+----------------------------------------------------------------+------------------------+------------------+----------------------------------------+--------------------------------+-------------+---------------+----------------+-------------------+\n",
            "|order_id                                                        |customer_id                                                     |order_created_at        |order_total_amount|item_name                               |item_id                         |item_quantity|item_unit_price|item_total_price|item_integration_id|\n",
            "+----------------------------------------------------------------+----------------------------------------------------------------+------------------------+------------------+----------------------------------------+--------------------------------+-------------+---------------+----------------+-------------------+\n",
            "|33e0612d62e5eb42aba15b58413137e441fbe906de2febd6a6c721b0e5773b44|7ba88a68bb2a3504c6bd37a707af57a0b8d6e110a551c719c9e7f71aaf3a99cf|2019-01-17T22:50:06.000Z|46.0              |Parmegiana de Fil√© de Frango (2 pessoas)|0bcd6764fd5e466d9c04b18ac0eb69e6|1.0          |2800.0         |2800.0          |PMFR               |\n",
            "|33e0612d62e5eb42aba15b58413137e441fbe906de2febd6a6c721b0e5773b44|7ba88a68bb2a3504c6bd37a707af57a0b8d6e110a551c719c9e7f71aaf3a99cf|2019-01-17T22:50:06.000Z|46.0              |Lasanha Frango (2 pessoas)              |a361f5eec6a44ac0817892e81bf22e80|1.0          |1800.0         |1800.0          |LF                 |\n",
            "|148c4353a2952f3fe7973547283265eb22b575fb712ed2618a6cf2d813b987ad|078acecdcf7fa89d356bfa349f14a8219db1ee161ce28ae4d9fee550c95d6fbd|2019-01-17T17:51:26.000Z|104.5             |Fil√© Mignon √† Cubana                    |e0e81b2027c241cca5bf60a4768800ec|1.0          |0.0            |0.0             |NULL               |\n",
            "|148c4353a2952f3fe7973547283265eb22b575fb712ed2618a6cf2d813b987ad|078acecdcf7fa89d356bfa349f14a8219db1ee161ce28ae4d9fee550c95d6fbd|2019-01-17T17:51:26.000Z|104.5             |603- Pudim de leite                     |4436f76edc164052aaf3c21a6567b570|1.0          |800.0          |800.0           |NULL               |\n",
            "|148c4353a2952f3fe7973547283265eb22b575fb712ed2618a6cf2d813b987ad|078acecdcf7fa89d356bfa349f14a8219db1ee161ce28ae4d9fee550c95d6fbd|2019-01-17T17:51:26.000Z|104.5             |601-Torta Holandesa                     |c0b788e876c54f198307dc142fed7aff|1.0          |800.0          |800.0           |NULL               |\n",
            "+----------------------------------------------------------------+----------------------------------------------------------------+------------------------+------------------+----------------------------------------+--------------------------------+-------------+---------------+----------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, from_json, explode\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# 1 - Definindo o schema do array de itens\n",
        "item_schema = ArrayType(\n",
        "    StructType([\n",
        "        StructField(\"name\", StringType()),\n",
        "        StructField(\"externalId\", StringType()),\n",
        "        StructField(\"quantity\", DoubleType()),\n",
        "        StructField(\"integrationId\", StringType()),\n",
        "        StructField(\"unitPrice\", StructType([\n",
        "            StructField(\"value\", StringType()),\n",
        "            StructField(\"currency\", StringType())\n",
        "        ])),\n",
        "        StructField(\"totalValue\", StructType([\n",
        "            StructField(\"value\", StringType()),\n",
        "            StructField(\"currency\", StringType())\n",
        "        ]))\n",
        "    ])\n",
        ")\n",
        "\n",
        "# 2 - Convertendo a string JSON para array de structs\n",
        "df_order_fixed = df_order.withColumn(\"items_array\", from_json(col(\"items\"), item_schema))\n",
        "\n",
        "# 3 - Aplicando explode na nova coluna\n",
        "df_exploded = df_order_fixed.withColumn(\"item\", explode(\"items_array\"))\n",
        "\n",
        "# 4 - Selecionando os campos desejados\n",
        "df_items_detalhados = df_exploded.select(\n",
        "    \"order_id\",\n",
        "    \"customer_id\",\n",
        "    \"order_created_at\",\n",
        "    \"order_total_amount\",\n",
        "    col(\"item.name\").alias(\"item_name\"),\n",
        "    col(\"item.externalId\").alias(\"item_id\"),\n",
        "    col(\"item.quantity\").alias(\"item_quantity\"),\n",
        "    col(\"item.unitPrice.value\").cast(\"float\").alias(\"item_unit_price\"),\n",
        "    col(\"item.totalValue.value\").cast(\"float\").alias(\"item_total_price\"),\n",
        "    col(\"item.integrationId\").alias(\"item_integration_id\")\n",
        ")\n",
        "\n",
        "# 5 - Exibindo resultado\n",
        "df_items_detalhados.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpFGp2zpjXWK"
      },
      "source": [
        "### Database Usu√°rios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMPO4ow1jayv",
        "outputId": "7661b6e0-fe46-4d10-bacd-d3785af38e2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Baixando o arquivo consumer.csv.gz...\n",
            "‚úÖ Download conclu√≠do!\n",
            "\n",
            "üìä Lendo o arquivo CSV com PySpark...\n",
            "‚úÖ Arquivo carregado em um DataFrame Spark!\n",
            "\n",
            "üîç Primeiras linhas do dataset:\n",
            "+----------------------------------------------------------------+--------+-----------------------+------+-------------+-------------------+---------------------+\n",
            "|customer_id                                                     |language|created_at             |active|customer_name|customer_phone_area|customer_phone_number|\n",
            "+----------------------------------------------------------------+--------+-----------------------+------+-------------+-------------------+---------------------+\n",
            "|e8cc60860e09c0bb19610b06ced69c973eb83982cfc98e397ce65cba92f70928|pt-br   |2018-04-05 14:49:18.165|true  |NUNO         |46                 |816135924            |\n",
            "|a2834a38a9876cf74e016524dd2e8c1f010ee12b2b684d58c40ab11eef19b6eb|pt-br   |2018-01-14 21:40:02.141|true  |ADRIELLY     |59                 |231330577            |\n",
            "|41e1051728eba13341136d67d0757f8d8cc44b2a405b718a8c5efba2c093b2c0|pt-br   |2018-01-07 03:47:15.554|true  |PAULA        |62                 |347597883            |\n",
            "|8e7c1dcb64edf95c935147f6d560cb068c44714cb1b21b287f1f433ae7b0b04e|pt-br   |2018-01-10 22:17:08.16 |true  |HELTON       |13                 |719366842            |\n",
            "|7823d4cf4150c5daeae0bd799206852fc123bdef0cf5d1639c8cb568d73e8c83|pt-br   |2018-04-06 00:16:20.935|true  |WENDER       |76                 |543232158            |\n",
            "+----------------------------------------------------------------+--------+-----------------------+------+-------------+-------------------+---------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "üß† Estrutura geral do dataset:\n",
            "root\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- language: string (nullable = true)\n",
            " |-- created_at: timestamp (nullable = true)\n",
            " |-- active: boolean (nullable = true)\n",
            " |-- customer_name: string (nullable = true)\n",
            " |-- customer_phone_area: integer (nullable = true)\n",
            " |-- customer_phone_number: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1 - Baixando o arquivo CSV compactado (.gz) da internet\n",
        "print(\"üì• Baixando o arquivo consumer.csv.gz...\")\n",
        "import urllib.request\n",
        "\n",
        "url = \"https://data-architect-test-source.s3-sa-east-1.amazonaws.com/consumer.csv.gz\"\n",
        "local_path = \"consumer.csv.gz\"\n",
        "urllib.request.urlretrieve(url, local_path)\n",
        "print(\"‚úÖ Download conclu√≠do!\\n\")\n",
        "\n",
        "# 2 - Lendo o arquivo CSV com PySpark\n",
        "print(\"üìä Lendo o arquivo CSV com PySpark...\")\n",
        "df_consumer = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(local_path)\n",
        "print(\"‚úÖ Arquivo carregado em um DataFrame Spark!\\n\")\n",
        "\n",
        "# 3 - Exibindo as primeiras linhas do DataFrame\n",
        "print(\"üîç Primeiras linhas do dataset:\")\n",
        "df_consumer.show(5, truncate=False)\n",
        "\n",
        "# 4 - Exibindo o schema do DataFrame\n",
        "print(\"\\nüß† Estrutura geral do dataset:\")\n",
        "df_consumer.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_lFsFpUeNX7"
      },
      "source": [
        "### Database Merchants - Restaurantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxV9Ekt4i-JB",
        "outputId": "0968b5a7-a29b-4e60-9ac9-7f5e4610b2ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Baixando o arquivo restaurant.csv.gz...\n",
            "‚úÖ Download conclu√≠do!\n",
            "\n",
            "üìä Lendo o arquivo CSV com PySpark...\n",
            "‚úÖ Arquivo carregado em um DataFrame Spark!\n",
            "\n",
            "üîç Primeiras linhas do dataset:\n",
            "+----------------------------------------------------------------+-----------------------+-------+-----------+--------------+------------+-------------+-------------------+-----------------+--------------+--------------+----------------+\n",
            "|id                                                              |created_at             |enabled|price_range|average_ticket|takeout_time|delivery_time|minimum_order_value|merchant_zip_code|merchant_city |merchant_state|merchant_country|\n",
            "+----------------------------------------------------------------+-----------------------+-------+-----------+--------------+------------+-------------+-------------------+-----------------+--------------+--------------+----------------+\n",
            "|d19ff6fca6288939bff073ad0a119d25c0365c407e9e5dd999e7a3e53c6d5d76|2017-01-23 12:52:30.91 |false  |3          |60.0          |0           |50           |30.0               |14025            |RIBEIRAO PRETO|SP            |BR              |\n",
            "|631df0985fdbbaf27b9b031a8f381924e3483833385748c8f7e1a41a6891d2b9|2017-01-20 13:14:48.286|true   |3          |60.0          |0           |0            |30.0               |50180            |SAO PAULO     |SP            |BR              |\n",
            "|135c5c4ae4c1ec1fdc23e8c649f313e39be8db913d8bc5aaff9bfddb548f096d|2017-01-23 12:46:33.457|true   |5          |100.0         |0           |45           |10.0               |23090            |RIO DE JANEIRO|RJ            |BR              |\n",
            "|d26f84c470451f752bef036c55517b6d9950d41806f10e94e8b5734241468768|2017-01-20 13:15:04.806|true   |3          |80.0          |0           |0            |18.9               |40255            |SALVADOR      |BA            |BR              |\n",
            "|97b9884600ea7192314580d9115f8882b8634f5aa201ff9a6df206e49343a283|2017-01-20 13:14:27.701|true   |3          |60.0          |0           |0            |25.0               |64600            |BARUERI       |SP            |BR              |\n",
            "+----------------------------------------------------------------+-----------------------+-------+-----------+--------------+------------+-------------+-------------------+-----------------+--------------+--------------+----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "üß† Estrutura geral do dataset:\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- created_at: timestamp (nullable = true)\n",
            " |-- enabled: boolean (nullable = true)\n",
            " |-- price_range: integer (nullable = true)\n",
            " |-- average_ticket: double (nullable = true)\n",
            " |-- takeout_time: integer (nullable = true)\n",
            " |-- delivery_time: integer (nullable = true)\n",
            " |-- minimum_order_value: double (nullable = true)\n",
            " |-- merchant_zip_code: integer (nullable = true)\n",
            " |-- merchant_city: string (nullable = true)\n",
            " |-- merchant_state: string (nullable = true)\n",
            " |-- merchant_country: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1 - Baixando o arquivo CSV compactado (.gz) da internet\n",
        "print(\"üì• Baixando o arquivo restaurant.csv.gz...\")\n",
        "import urllib.request\n",
        "\n",
        "url = \"https://data-architect-test-source.s3-sa-east-1.amazonaws.com/restaurant.csv.gz\"\n",
        "local_path = \"restaurant.csv.gz\"\n",
        "urllib.request.urlretrieve(url, local_path)\n",
        "print(\"‚úÖ Download conclu√≠do!\\n\")\n",
        "\n",
        "# 2 - Lendo o arquivo CSV com PySpark\n",
        "print(\"üìä Lendo o arquivo CSV com PySpark...\")\n",
        "df_restaurant = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(local_path)\n",
        "print(\"‚úÖ Arquivo carregado em um DataFrame Spark!\\n\")\n",
        "\n",
        "# 3 - Exibindo as primeiras linhas do DataFrame\n",
        "print(\"üîç Primeiras linhas do dataset:\")\n",
        "df_restaurant.show(5, truncate=False)\n",
        "\n",
        "# 4 - Exibindo o schema do DataFrame\n",
        "print(\"\\nüß† Estrutura geral do dataset:\")\n",
        "df_restaurant.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gye7UAj6d-V1"
      },
      "source": [
        "### Database Marca√ß√£o de usu√°rios que participaram do teste A/B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdghaOfreqlG",
        "outputId": "f648a303-dcc2-4f30-8877-969a861b1062"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "openjdk-11-jdk is already the newest version (11.0.27+6~us1-0ubuntu1~22.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "‚úÖ Spark iniciado!\n",
            "\n",
            "üì• Baixando o arquivo ab_test_ref.tar.gz...\n",
            "‚úÖ Download conclu√≠do!\n",
            "\n",
            "üóÇ Extraindo arquivos...\n",
            "‚úÖ Extra√ß√£o conclu√≠da!\n",
            "\n",
            "üìÑ Lendo arquivo: ab_test_ref_extracted/ab_test_ref.csv\n",
            "üîç Primeiras linhas do DataFrame:\n",
            "+----------------------------------------------------------------+---------+\n",
            "|customer_id                                                     |is_target|\n",
            "+----------------------------------------------------------------+---------+\n",
            "|755e1fa18f25caec5edffb188b13fd844b2af8cf5adedcf77c028f36cb9382ea|target   |\n",
            "|b821aa8372b8e5b82cdc283742757df8c45eecdd72adf411716e710525d4edf1|control  |\n",
            "|d425d6ee4c9d4e211b71da8fc60bf6c5336b2ea9af9cc007f5297541ec40b63b|control  |\n",
            "|6a7089eea0a5dc294fbccd4fa24d0d84a90c1cc12e829c8b535718bbc651ab02|target   |\n",
            "|dad6b7e222bab31c0332b0ccd9fa5dbd147008facd268f5e3763fa657c23a58d|control  |\n",
            "+----------------------------------------------------------------+---------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "üß† Estrutura do DataFrame:\n",
            "root\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- is_target: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1 - Instalar Java e PySpark no Google Colab\n",
        "!apt-get install openjdk-11-jdk -y\n",
        "!pip install pyspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "# 2 - Iniciar SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"AB Test Case\").getOrCreate()\n",
        "print(\"‚úÖ Spark iniciado!\\n\")\n",
        "\n",
        "# 3 - Baixar o arquivo .tar.gz\n",
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "print(\"üì• Baixando o arquivo ab_test_ref.tar.gz...\")\n",
        "url = \"https://data-architect-test-source.s3-sa-east-1.amazonaws.com/ab_test_ref.tar.gz\"\n",
        "local_tar_path = \"ab_test_ref.tar.gz\"\n",
        "urllib.request.urlretrieve(url, local_tar_path)\n",
        "print(\"‚úÖ Download conclu√≠do!\\n\")\n",
        "\n",
        "# 4 - Extrair o conte√∫do\n",
        "print(\"üóÇ Extraindo arquivos...\")\n",
        "extract_path = \"ab_test_ref_extracted\"\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "with tarfile.open(local_tar_path, \"r:gz\") as tar:\n",
        "    tar.extractall(path=extract_path)\n",
        "print(\"‚úÖ Extra√ß√£o conclu√≠da!\\n\")\n",
        "\n",
        "# 5 - Caminho direto para o arquivo extra√≠do\n",
        "csv_path = os.path.join(extract_path, \"ab_test_ref.csv\")\n",
        "print(f\"üìÑ Lendo arquivo: {csv_path}\")\n",
        "\n",
        "# 6 - Definir schema do arquivo\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "schema = StructType([\n",
        "    StructField(\"customer_id\", StringType(), True),\n",
        "    StructField(\"is_target\", StringType(), True),\n",
        "])\n",
        "\n",
        "# 7 - Leitura do CSV com PySpark\n",
        "df_ab_test = spark.read.option(\"header\", True)\\\n",
        "                       .option(\"sep\", \",\")\\\n",
        "                       .schema(schema)\\\n",
        "                       .csv(csv_path)\n",
        "\n",
        "# 8 - Visualizar os dados\n",
        "print(\"üîç Primeiras linhas do DataFrame:\")\n",
        "df_ab_test.show(5, truncate=False)\n",
        "\n",
        "print(\"\\nüß† Estrutura do DataFrame:\")\n",
        "df_ab_test.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "facae9bd"
      },
      "source": [
        "**Explanation of the fix:**\n",
        "\n",
        "1.  **`items_schema`**: We define a `StructType` that matches the expected structure of each item within the `items` array. This includes nested `StructType` for fields like `addition`, `discount`, `unitPrice`, `totalValue`, `totalAddition`, and `totalDiscount`, and an `ArrayType` for `garnishItems`. I've used `StringType` for currency values and prices based on the example data in the output of cell `Bh7E-EBFhrMs`, and `LongType` for `sequence`.\n",
        "2.  **`from_json(col(\"items\"), items_schema)`**: This function parses the JSON string in the `items` column according to the `items_schema` we defined. The result is a new column named `parsed_items` which is of type `ArrayType(StructType(...))`.\n",
        "3.  **`explode(\"parsed_items\")`**: Now that `parsed_items` is an array type, we can successfully apply the `explode` function to it. This creates a new row for each item in the array.\n",
        "4.  **`df_exploded.select(\"order_id\", \"item.*\").show(5, truncate=False)`**: This line selects the original `order_id` and all the fields from the exploded `item` column to show the result of the explosion.\n",
        "\n",
        "This corrected code will successfully extract the individual items from the `items` column and create a new row for each item, allowing you to analyze the items within each order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hols3Z15zeOB"
      },
      "source": [
        "## O Desafio ‚Äì An√°lise do teste A/B (ETAPA 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCqxnpwEznQ9"
      },
      "source": [
        "‚úÖ C√ìDIGO PySpark: An√°lise A/B (impacto da campanha)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of998RTczf9j"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, countDistinct, count, sum, avg, when\n",
        "from pyspark.sql.window import Window\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW_-8tU6zvfl"
      },
      "outputs": [],
      "source": [
        "# 1 - Juntando pedidos com grupo de teste\n",
        "df_ab_orders = df_order.join(df_ab_test, on=\"customer_id\", how=\"inner\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hqQycZtzyaO"
      },
      "outputs": [],
      "source": [
        "# 2 - Calculando indicadores agregados por grupo (target vs control)\n",
        "df_metrics = df_ab_orders.groupBy(\"is_target\").agg(\n",
        "    countDistinct(\"customer_id\").alias(\"qtd_usuarios\"),\n",
        "    count(\"order_id\").alias(\"qtd_pedidos\"),\n",
        "    avg(\"order_total_amount\").alias(\"ticket_medio\"),\n",
        "    sum(\"order_total_amount\").alias(\"valor_total_pedidos\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aof8Us7G0PG0",
        "outputId": "d93ba985-6604-41cb-a0ed-b28677d757fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+------------+-----------+------------------+--------------------+-------------------+\n",
            "|is_target|qtd_usuarios|qtd_pedidos|ticket_medio      |valor_total_pedidos |pedidos_por_usuario|\n",
            "+---------+------------+-----------+------------------+--------------------+-------------------+\n",
            "|control  |360542      |1525576    |47.897890947419995|7.30718728800012E7  |4.231340592774212  |\n",
            "|target   |445924      |2136745    |47.73970213572941 |1.0200756984000914E8|4.79172459881056   |\n",
            "+---------+------------+-----------+------------------+--------------------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3 - Calculando pedidos por usu√°rio (frequ√™ncia m√©dia)\n",
        "df_metrics = df_metrics.withColumn(\n",
        "    \"pedidos_por_usuario\",\n",
        "    col(\"qtd_pedidos\") / col(\"qtd_usuarios\")\n",
        ")\n",
        "\n",
        "# 4 - Exibindo resultados\n",
        "df_metrics.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H11cXAo57BQ"
      },
      "source": [
        "### a) Definindo os indicadores relevantes para mensurar o sucesso da\n",
        "campanha e analisar se ela teve impacto significativo dentro do\n",
        "per√≠odo avaliado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc1dtBNl2Ts0",
        "outputId": "2ab1fa4e-7f4e-49be-fd8c-ff482bf095cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "qtd_usuarios qtd_pedidos ticket_medio valor_total_pedidos pedidos_por_usuario\n",
            "     360.542   1.525.576     R$ 47,90    R$ 73.071.872,88                 4,2\n",
            "     445.924   2.136.745     R$ 47,74   R$ 102.007.569,84                 4,8\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, countDistinct, count, sum, avg\n",
        "import pandas as pd\n",
        "\n",
        "# 1 - Juntando pedidos com grupo de teste\n",
        "df_ab_orders = df_order.join(df_ab_test, on=\"customer_id\", how=\"inner\")\n",
        "\n",
        "# 2 - Calculando indicadores agregados por grupo (target vs control)\n",
        "df_metrics = df_ab_orders.groupBy(\"is_target\").agg(\n",
        "    countDistinct(\"customer_id\").alias(\"qtd_usuarios\"),\n",
        "    count(\"order_id\").alias(\"qtd_pedidos\"),\n",
        "    avg(\"order_total_amount\").alias(\"ticket_medio\"),\n",
        "    sum(\"order_total_amount\").alias(\"valor_total_pedidos\")\n",
        ")\n",
        "\n",
        "# 3 - Calculando pedidos por usu√°rio\n",
        "df_metrics = df_metrics.withColumn(\n",
        "    \"pedidos_por_usuario\",\n",
        "    col(\"qtd_pedidos\") / col(\"qtd_usuarios\")\n",
        ")\n",
        "\n",
        "# 4 - Convertendo para Pandas para aplicar a formata√ß√£o personalizada\n",
        "df_metrics_pd = df_metrics.toPandas()\n",
        "\n",
        "# 5 - Formatando os valores para estilo brasileiro\n",
        "df_metrics_formatado = pd.DataFrame()\n",
        "#df_metrics_formatado[\"Grupo\"] = df_metrics_pd[\"is_target\"]\n",
        "\n",
        "df_metrics_formatado[\"qtd_usuarios\"] = df_metrics_pd[\"qtd_usuarios\"].apply(lambda x: f\"{x:,.0f}\".replace(\",\", \".\"))\n",
        "df_metrics_formatado[\"qtd_pedidos\"] = df_metrics_pd[\"qtd_pedidos\"].apply(lambda x: f\"{x:,.0f}\".replace(\",\", \".\"))\n",
        "df_metrics_formatado[\"ticket_medio\"] = df_metrics_pd[\"ticket_medio\"].apply(lambda x: f\"R$ {x:,.2f}\".replace(\",\", \"X\").replace(\".\", \",\").replace(\"X\", \".\"))\n",
        "df_metrics_formatado[\"valor_total_pedidos\"] = df_metrics_pd[\"valor_total_pedidos\"].apply(lambda x: f\"R$ {x:,.2f}\".replace(\",\", \"X\").replace(\".\", \",\").replace(\"X\", \".\"))\n",
        "df_metrics_formatado[\"pedidos_por_usuario\"] = df_metrics_pd[\"pedidos_por_usuario\"].apply(lambda x: f\"{x:.1f}\".replace(\".\", \",\"))\n",
        "\n",
        "# 6 - Exibindo resultado\n",
        "print(df_metrics_formatado.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTOr4SQVtBPf"
      },
      "source": [
        "‚úÖ 1. Join dos dados de pedidos com teste A/B e restaurantes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBtv_guZtFJN"
      },
      "outputs": [],
      "source": [
        "# Base principal de an√°lise\n",
        "df_ab_orders = df_order.join(df_ab_test, on=\"customer_id\", how=\"inner\")\n",
        "df_ab_orders = df_ab_orders.join(df_restaurant, df_ab_orders[\"merchant_id\"] == df_restaurant[\"id\"], \"inner\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq1vHIf1tHqa"
      },
      "source": [
        "üìä 2. KPIs principais por grupo (usu√°rios e pedidos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfyI7uSBtKx8"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, countDistinct, count, sum, avg\n",
        "\n",
        "df_metrics = df_ab_orders.groupBy(\"is_target\").agg(\n",
        "    countDistinct(\"customer_id\").alias(\"qtd_usuarios\"),\n",
        "    count(\"order_id\").alias(\"qtd_pedidos\"),\n",
        "    sum(\"order_total_amount\").alias(\"valor_total_pedidos\"),\n",
        "    avg(\"order_total_amount\").alias(\"ticket_medio\")\n",
        ").withColumn(\n",
        "    \"pedidos_por_usuario\", col(\"qtd_pedidos\") / col(\"qtd_usuarios\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y50CyjgAtQtk"
      },
      "source": [
        "üîÅ 3. Reten√ß√£o de usu√°rios (usu√°rios com >1 pedido)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLYoObQntTOR"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when, round\n",
        "\n",
        "df_freq = df_ab_orders.groupBy(\"is_target\", \"customer_id\").agg(count(\"order_id\").alias(\"qtd_pedidos\"))\n",
        "\n",
        "df_retencao = df_freq.groupBy(\"is_target\").agg(\n",
        "    count(\"*\").alias(\"total_usuarios\"),\n",
        "    count(when(col(\"qtd_pedidos\") > 1, True)).alias(\"usuarios_retidos\")\n",
        ").withColumn(\n",
        "    \"taxa_retencao\", round((col(\"usuarios_retidos\") / col(\"total_usuarios\")) * 100, 2)\n",
        ").select(\"is_target\", \"taxa_retencao\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SME_og03tWHI"
      },
      "source": [
        "üìà 4. Incremento percentual do ticket m√©dio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a21p1cUDtYR8"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import first\n",
        "\n",
        "df_ticket_medio = df_ab_orders.groupBy(\"is_target\").agg(round(avg(\"order_total_amount\"), 2).alias(\"ticket_medio\"))\n",
        "\n",
        "df_incremento = df_ticket_medio.groupBy().pivot(\"is_target\").agg(first(\"ticket_medio\")).withColumn(\n",
        "    \"incremento_percentual\", round(((col(\"target\") - col(\"control\")) / col(\"control\")) * 100, 2)\n",
        ").select(\"incremento_percentual\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrapWsHutaOc"
      },
      "source": [
        "üîù 5. Heavy users (usu√°rios que est√£o no top 20% de pedidos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUxAX8Mqtclu"
      },
      "outputs": [],
      "source": [
        "df_user_freq = df_ab_orders.groupBy(\"customer_id\").agg(count(\"order_id\").alias(\"qtd_pedidos\"))\n",
        "percentil_80 = df_user_freq.approxQuantile(\"qtd_pedidos\", [0.80], 0.01)[0]\n",
        "\n",
        "df_user_freq = df_user_freq.withColumn(\n",
        "    \"heavy_user\", when(col(\"qtd_pedidos\") >= percentil_80, 1).otherwise(0)\n",
        ")\n",
        "\n",
        "df_heavy = df_user_freq.join(df_ab_test, on=\"customer_id\", how=\"inner\")\n",
        "\n",
        "df_heavy_summary = df_heavy.groupBy(\"is_target\").agg(\n",
        "    count(when(col(\"heavy_user\") == 1, True)).alias(\"qtd_heavy_users\"),\n",
        "    count(\"*\").alias(\"total_usuarios\")\n",
        ").withColumn(\n",
        "    \"percentual_heavy_users\", round((col(\"qtd_heavy_users\") / col(\"total_usuarios\")) * 100, 2)\n",
        ").select(\"is_target\", \"percentual_heavy_users\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql4db6lGte-k"
      },
      "source": [
        "üïí 6. Tempo m√©dio de entrega por grupo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5Cf5uUmthWI"
      },
      "outputs": [],
      "source": [
        "df_delivery_time = df_ab_orders.groupBy(\"is_target\").agg(\n",
        "    round(avg(\"delivery_time\"), 2).alias(\"tempo_medio_entrega\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3qfO5GOtttS"
      },
      "source": [
        "üçΩÔ∏è 7. Total de restaurantes por grupo (com base em pedidos)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2BHGmjCtwAt"
      },
      "outputs": [],
      "source": [
        "df_qtd_rest = df_ab_orders.groupBy(\"is_target\").agg(\n",
        "    countDistinct(\"merchant_id\").alias(\"qtd_restaurantes\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnRD_iMitzt8"
      },
      "source": [
        "üì¶ 8. M√©dia de pedidos por restaurante"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGuhzUilt2b2"
      },
      "outputs": [],
      "source": [
        "df_avg_pedidos_rest = df_ab_orders.groupBy(\"is_target\", \"merchant_id\").agg(\n",
        "    count(\"order_id\").alias(\"qtd_pedidos_rest\")\n",
        ").groupBy(\"is_target\").agg(\n",
        "    round(avg(\"qtd_pedidos_rest\"), 2).alias(\"media_pedidos_por_restaurante\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLtKMwchuC6U"
      },
      "source": [
        "üßæ 10. Juntando todos os KPIs em uma tabela final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6hIcsQWuHAh",
        "outputId": "5fdc5845-2ef7-49dc-e4bf-5eb118abb8b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+-----------+--------------------+------------------+-------------------+--------------+----------------+-------------+----------------------+-------------------+----------------+-----------------------------+\n",
            "|is_target|qtd_usuarios|qtd_pedidos|valor_total_pedidos |ticket_medio      |pedidos_por_usuario|total_usuarios|usuarios_retidos|taxa_retencao|percentual_heavy_users|tempo_medio_entrega|qtd_restaurantes|media_pedidos_por_restaurante|\n",
            "+---------+------------+-----------+--------------------+------------------+-------------------+--------------+----------------+-------------+----------------------+-------------------+----------------+-----------------------------+\n",
            "|control  |360542      |1525576    |7.307187305451663E7 |47.897891061813134|4.231340592774212  |360542        |269341          |74.7         |21.53                 |22.63              |7196            |212.0                        |\n",
            "|target   |445924      |2136745    |1.0200757006332143E8|47.73970224023991 |4.79172459881056   |445924        |354538          |79.51        |25.87                 |22.66              |7227            |295.66                       |\n",
            "+---------+------------+-----------+--------------------+------------------+-------------------+--------------+----------------+-------------+----------------------+-------------------+----------------+-----------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Juntando tudo com PySpark\n",
        "df_final = df_metrics \\\n",
        "    .join(df_retencao, on=\"is_target\", how=\"left\") \\\n",
        "    .join(df_heavy_summary, on=\"is_target\", how=\"left\") \\\n",
        "    .join(df_delivery_time, on=\"is_target\", how=\"left\") \\\n",
        "    .join(df_qtd_rest, on=\"is_target\", how=\"left\") \\\n",
        "    .join(df_avg_pedidos_rest, on=\"is_target\", how=\"left\")\n",
        "\n",
        "df_final.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üìä An√°lise Comparativa ‚Äî Grupo Controle vs Grupo Target"
      ],
      "metadata": {
        "id": "zAOpZVGF_qy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Indicador                    | Controle  | Target    | Diferen√ßa | Interpreta√ß√£o                               |\n",
        "| ---------------------------- | --------- | --------- | --------- | ------------------------------------------- |\n",
        "| **Usu√°rios (qtd\\_usuarios)** | 360.542   | 445.924   | +85.382   | Target teve uma base maior.                 |\n",
        "| **Pedidos (qtd\\_pedidos)**   | 1.525.576 | 2.136.745 | +611.169  | Target pediu mais.                          |\n",
        "| **Ticket m√©dio**             | R\\$ 47,90 | R\\$ 47,74 | -R\\$ 0,16 | Leve queda no valor por pedido no target.   |\n",
        "| **Pedidos por usu√°rio**      | 4,23      | 4,79      | +13,2%    | Mais engajamento no target.                 |\n",
        "| **Reten√ß√£o de usu√°rios (%)** | 74,7%     | 79,51%    | +4,8 p.p. | A campanha reteve melhor os usu√°rios.       |\n",
        "| **Heavy Users (%)**          | 21,53%    | 25,87%    | +4,3 p.p. | Mais usu√°rios muito ativos no target.       |\n",
        "| **Tempo m√©dio de entrega**   | 22,63 min | 22,66 min | ‚âà         | N√£o h√° impacto log√≠stico relevante.         |\n",
        "| **Pedidos por restaurante**  | 212,0     | 295,66    | +39,4%    | Restaurantes no grupo target venderam mais. |\n"
      ],
      "metadata": {
        "id": "o2Ua9TzzB6Zk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**An√°lise de Engajamento e Produto:**\n",
        "\n",
        "Observando as m√©tricas de comportamento, a campanha foi um sucesso absoluto.\n",
        "\n",
        "\n",
        "*   Frequ√™ncia de Pedidos: Os usu√°rios do grupo target fizeram, em m√©dia, 4,79 pedidos, um aumento de 13,2% em rela√ß√£o aos 4,23 do grupo control. Isso mostra que o incentivo funcionou para criar o h√°bito de pedir mais vezes.\n",
        "\n",
        "*   Taxa de Reten√ß√£o: A reten√ß√£o do grupo target foi de 79,51%, quase 5 pontos percentuais maior que a do grupo control. Este √© um ganho massivo e o indicador mais forte de que a campanha gerou lealdade.\n",
        "\n",
        "\n",
        "*   \"Heavy Users\": O percentual de usu√°rios de alta frequ√™ncia subiu de 21,53% para 25,87%. A campanha foi eficaz em converter usu√°rios e engajar a base.\n",
        "\n",
        "\n",
        "Conclus√£o de Produto: Sem d√∫vida, a campanha foi um sucesso em engajar a base de usu√°rios, aumentar a frequ√™ncia e reter mais clientes."
      ],
      "metadata": {
        "id": "cJ7kuYtFCThI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b) An√°lise da viabilidade financeira dessa iniciativa\n",
        "como alavanca de crescimento"
      ],
      "metadata": {
        "id": "rCCOy4t_A9yj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üí∞ An√°lise da Viabilidade Financeira\n"
      ],
      "metadata": {
        "id": "VS2bMlLLFVBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Premissas Adotadas:**\n",
        "\n",
        "*   Custo da Campanha: R$ 10,00 por cada pedido realizado no grupo target.\n",
        "*   Take Rate (Receita do iFood): Manteremos a premissa de 20% sobre o valor total dos pedidos (GMV).\n"
      ],
      "metadata": {
        "id": "dEJUbXyYFek8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1 - Custo da Campanha**"
      ],
      "metadata": {
        "id": "Wnbm-Fk-Fynu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# --- Premissas ---\n",
        "# Ajuste o valor do custo do cupom conforme a sua necessidade.\n",
        "CUSTO_CUPOM_POR_PEDIDO = 10.00\n",
        "\n",
        "# --- C√°lculo Direto ---\n",
        "# O c√≥digo filtra o DataFrame para o grupo 'target', conta o n√∫mero de pedidos\n",
        "# e multiplica pelo custo unit√°rio do cupom para obter o valor total.\n",
        "custo_total_campanha = df_ab_orders.filter(col(\"is_target\") == \"target\").count() * CUSTO_CUPOM_POR_PEDIDO\n",
        "\n",
        "# --- Resultado ---\n",
        "print(f\"Custo Total: R$ {custo_total_campanha:,.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mW4_1TZfFyFD",
        "outputId": "6565e697-a81d-4098-a44b-a9e7885488c1"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custo Total: R$ 21,367,450.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2 - Receita Incremental**"
      ],
      "metadata": {
        "id": "ZAPIjd8nGGdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum, countDistinct\n",
        "\n",
        "# --- Premissas ---\n",
        "# Ajuste o Take Rate (comiss√£o do iFood estimada).\n",
        "TAKE_RATE = 0.20\n",
        "\n",
        "# --- 1. Agrega√ß√£o com PySpark ---\n",
        "# Agrupamos os dados para obter os totais de usu√°rios e GMV para cada grupo.\n",
        "summary_df = df_ab_orders.groupBy(\"is_target\").agg(\n",
        "    countDistinct(\"customer_id\").alias(\"qtd_usuarios\"),\n",
        "    sum(\"order_total_amount\").alias(\"gmv_total\")\n",
        ")\n",
        "\n",
        "# --- 2. Extra√ß√£o dos Dados Agregados ---\n",
        "# Coletamos o pequeno DataFrame de resumo para o Python para facilitar os c√°lculos.\n",
        "# O .collect() aqui √© seguro e eficiente, pois o resultado tem apenas 2 linhas.\n",
        "metrics = {row[\"is_target\"]: row for row in summary_df.collect()}\n",
        "\n",
        "control_metrics = metrics.get(\"control\")\n",
        "target_metrics = metrics.get(\"target\")\n",
        "\n",
        "# --- 3. C√°lculo Financeiro ---\n",
        "# Verificamos se ambos os grupos existem antes de calcular\n",
        "if control_metrics and target_metrics:\n",
        "    # Comportamento base: Qual foi o GMV por usu√°rio no grupo de controle?\n",
        "    gmv_por_usuario_control = control_metrics[\"gmv_total\"] / control_metrics[\"qtd_usuarios\"]\n",
        "\n",
        "    # Proje√ß√£o: Qual seria o GMV do grupo Target se eles fossem como o Controle?\n",
        "    gmv_baseline_target = gmv_por_usuario_control * target_metrics[\"qtd_usuarios\"]\n",
        "\n",
        "    # Receita projetada (Baseline) vs. Receita Real\n",
        "    receita_baseline_target = gmv_baseline_target * TAKE_RATE\n",
        "    receita_real_target = target_metrics[\"gmv_total\"] * TAKE_RATE\n",
        "\n",
        "    # A Receita Incremental √© a diferen√ßa entre o que aconteceu e o que teria acontecido.\n",
        "    receita_incremental = receita_real_target - receita_baseline_target\n",
        "else:\n",
        "    receita_incremental = 0  # Define como 0 se um dos grupos n√£o existir\n",
        "\n",
        "# --- 4. Resultado ---\n",
        "print(\"  C√ÅLCULO DA RECEITA INCREMENTAL DA CAMPANHA\")\n",
        "print(f\"Take Rate (premissa): {TAKE_RATE:.0%}\")\n",
        "print(\"---------------------------------------------\")\n",
        "print(f\"Receita Incremental Gerada: R$ {receita_incremental:,.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHC4G2ZsJjer",
        "outputId": "bd997b47-e209-4789-95ff-83eb22a64ccc"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C√ÅLCULO DA RECEITA INCREMENTAL DA CAMPANHA\n",
            "Take Rate (premissa): 20%\n",
            "---------------------------------------------\n",
            "Receita Incremental Gerada: R$ 2,326,226.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3 - Resultado L√≠quido**"
      ],
      "metadata": {
        "id": "ZNuM4HLLGKcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum, count, countDistinct\n",
        "\n",
        "# --- 1. Premissas da An√°lise ---\n",
        "# Ajuste estes valores conforme sua necessidade.\n",
        "TAKE_RATE = 0.20  # 20% de comiss√£o para o iFood\n",
        "CUSTO_CUPOM_POR_PEDIDO = 10.00  # R$ 10,00 por pedido no grupo Target\n",
        "\n",
        "# --- 2. Agrega√ß√£o √önica com PySpark ---\n",
        "# Em uma √∫nica passagem, calculamos todos os totais necess√°rios para ambos os grupos.\n",
        "summary_df = df_ab_orders.groupBy(\"is_target\").agg(\n",
        "    countDistinct(\"customer_id\").alias(\"qtd_usuarios\"),\n",
        "    count(\"order_id\").alias(\"qtd_pedidos\"),\n",
        "    sum(\"order_total_amount\").alias(\"gmv_total\")\n",
        ")\n",
        "\n",
        "# --- 3. Extra√ß√£o dos Dados para o Modelo Financeiro ---\n",
        "# Coletamos o pequeno DataFrame de resumo para o Python para facilitar os c√°lculos.\n",
        "metrics = {row[\"is_target\"]: row for row in summary_df.collect()}\n",
        "\n",
        "control_metrics = metrics.get(\"control\")\n",
        "target_metrics = metrics.get(\"target\")\n",
        "\n",
        "# --- 4. C√°lculo dos Componentes e do Resultado L√≠quido ---\n",
        "# Inicializamos as vari√°veis para evitar erros se um grupo n√£o existir\n",
        "receita_incremental = 0\n",
        "custo_total_campanha = 0\n",
        "\n",
        "if control_metrics and target_metrics:\n",
        "    # --- C√°lculo do Custo da Campanha ---\n",
        "    custo_total_campanha = target_metrics[\"qtd_pedidos\"] * CUSTO_CUPOM_POR_PEDIDO\n",
        "\n",
        "    # --- C√°lculo da Receita Incremental ---\n",
        "    gmv_por_usuario_control = control_metrics[\"gmv_total\"] / control_metrics[\"qtd_usuarios\"]\n",
        "    gmv_baseline_target = gmv_por_usuario_control * target_metrics[\"qtd_usuarios\"]\n",
        "    receita_baseline_target = gmv_baseline_target * TAKE_RATE\n",
        "    receita_real_target = target_metrics[\"gmv_total\"] * TAKE_RATE\n",
        "    receita_incremental = receita_real_target - receita_baseline_target\n",
        "\n",
        "# --- C√°lculo Final ---\n",
        "resultado_liquido = receita_incremental - custo_total_campanha\n",
        "\n",
        "\n",
        "# --- 5. Exibi√ß√£o do Resultado ---\n",
        "print(\"COMPONENTES DA AN√ÅLISE:\")\n",
        "print(f\"  (+) Receita Incremental Gerada: R$ {receita_incremental:,.2f}\")\n",
        "print(f\"  (-) Custo Total da Campanha:    R$ {custo_total_campanha:,.2f}\")\n",
        "print(\"--------------------------------------------------\")\n",
        "print(f\"  üìä Resultado L√≠quido Final:    R$ {resultado_liquido:,.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUaML8npGMQT",
        "outputId": "a8b30f4a-6dbe-4560-b15f-62b212e136ed"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMPONENTES DA AN√ÅLISE:\n",
            "  (+) Receita Incremental Gerada: R$ 2,326,226.30\n",
            "  (-) Custo Total da Campanha:    R$ 21,367,450.00\n",
            "--------------------------------------------------\n",
            "  üìä Resultado L√≠quido Final:    R$ -19,041,223.70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4 - ROI: Retorno sobre Investimento**"
      ],
      "metadata": {
        "id": "AM9TlMQpGN4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum, count, countDistinct\n",
        "\n",
        "# --- 1. Premissas da An√°lise ---\n",
        "# Ajuste estes valores conforme sua necessidade.\n",
        "TAKE_RATE = 0.20  # 20% de comiss√£o para o iFood\n",
        "CUSTO_CUPOM_POR_PEDIDO = 10.00  # R$ 10,00 por pedido no grupo Target\n",
        "\n",
        "# --- 2. Agrega√ß√£o √önica com PySpark ---\n",
        "# Em uma √∫nica passagem, calculamos todos os totais necess√°rios para ambos os grupos.\n",
        "summary_df = df_ab_orders.groupBy(\"is_target\").agg(\n",
        "    countDistinct(\"customer_id\").alias(\"qtd_usuarios\"),\n",
        "    count(\"order_id\").alias(\"qtd_pedidos\"),\n",
        "    sum(\"order_total_amount\").alias(\"gmv_total\")\n",
        ")\n",
        "\n",
        "# --- 3. Extra√ß√£o dos Dados para o Modelo Financeiro ---\n",
        "# Coletamos o pequeno DataFrame de resumo para o Python para facilitar os c√°lculos.\n",
        "metrics = {row[\"is_target\"]: row for row in summary_df.collect()}\n",
        "\n",
        "control_metrics = metrics.get(\"control\")\n",
        "target_metrics = metrics.get(\"target\")\n",
        "\n",
        "# --- 4. C√°lculo dos Componentes Financeiros ---\n",
        "# Inicializamos as vari√°veis para evitar erros se um grupo n√£o existir\n",
        "receita_incremental = 0\n",
        "custo_total_campanha = 0\n",
        "resultado_liquido = 0\n",
        "roi = 0\n",
        "\n",
        "if control_metrics and target_metrics:\n",
        "    # --- Custo da Campanha ---\n",
        "    custo_total_campanha = target_metrics[\"qtd_pedidos\"] * CUSTO_CUPOM_POR_PEDIDO\n",
        "\n",
        "    # --- Receita Incremental ---\n",
        "    gmv_por_usuario_control = control_metrics[\"gmv_total\"] / control_metrics[\"qtd_usuarios\"]\n",
        "    gmv_baseline_target = gmv_por_usuario_control * target_metrics[\"qtd_usuarios\"]\n",
        "    receita_baseline_target = gmv_baseline_target * TAKE_RATE\n",
        "    receita_real_target = target_metrics[\"gmv_total\"] * TAKE_RATE\n",
        "    receita_incremental = receita_real_target - receita_baseline_target\n",
        "\n",
        "    # --- Resultado L√≠quido ---\n",
        "    resultado_liquido = receita_incremental - custo_total_campanha\n",
        "\n",
        "    # --- C√°lculo Final do ROI ---\n",
        "    # Verificamos se o custo √© maior que zero para evitar divis√£o por zero\n",
        "    if custo_total_campanha > 0:\n",
        "        roi = resultado_liquido / custo_total_campanha\n",
        "\n",
        "# --- 5. Exibi√ß√£o do Relat√≥rio Financeiro Completo ---\n",
        "print(\"PREMISSAS ADOTADAS:\")\n",
        "print(f\"  - Take Rate (Comiss√£o iFood): {TAKE_RATE:.0%}\")\n",
        "print(f\"  - Custo do Cupom por Pedido (Target): R$ {CUSTO_CUPOM_POR_PEDIDO:.2f}\\n\")\n",
        "\n",
        "print(\"AN√ÅLISE DE RESULTADOS:\")\n",
        "print(f\"  - Receita Incremental Gerada:       R$ {receita_incremental:,.2f}\")\n",
        "print(f\"  - Custo Total da Campanha:          R$ {custo_total_campanha:,.2f}\")\n",
        "print(\"-------------------------------------------------------\")\n",
        "print(f\"  üìä Resultado L√≠quido Final:          R$ {resultado_liquido:,.2f}\")\n",
        "print(\"-------------------------------------------------------\")\n",
        "print(f\"  üìà ROI (Retorno sobre Investimento): {roi:.2%}\")\n",
        "print(\"=\"*55)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ytpm7NrGQL7",
        "outputId": "ca2cf051-1a2b-40dd-9c95-c5891740de2a"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREMISSAS ADOTADAS:\n",
            "  - Take Rate (Comiss√£o iFood): 20%\n",
            "  - Custo do Cupom por Pedido (Target): R$ 10.00\n",
            "\n",
            "AN√ÅLISE DE RESULTADOS:\n",
            "  - Receita Incremental Gerada:       R$ 2,326,226.30\n",
            "  - Custo Total da Campanha:          R$ 21,367,450.00\n",
            "-------------------------------------------------------\n",
            "  üìä Resultado L√≠quido Final:          R$ -19,041,223.70\n",
            "-------------------------------------------------------\n",
            "  üìà ROI (Retorno sobre Investimento): -89.11%\n",
            "=======================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìâ Conclus√£o: A Campanha Teve Retorno?**\n",
        "\n",
        "\n",
        "N√£o. A campanha **n√£o teve retorno financeiro positivo.**\n",
        "\n",
        "**Apesar de gerar:**\n",
        "\n",
        "*   Maior engajamento dos usu√°rios (mais pedidos por usu√°rio)\n",
        "*   Maior taxa de reten√ß√£o\n",
        "*   Mais heavy users\n",
        "*   Aumento nas vendas dos restaurantes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "üëâ O custo da campanha (desconto) foi quase 10x maior que a receita adicional gerada, gerando **preju√≠zo** de mais de R$ 19 milh√µes e um ROI negativo de -89,11%.\n",
        "\n",
        "\n",
        "üí° Insight\n",
        "A estrat√©gia melhorou comportamento e engajamento, mas n√£o foi eficiente financeiramente.\n",
        "\n",
        "Se o cupom fosse menor (ex: R$5) ou vinculado a pedidos acima de um valor m√≠nimo de R\\$ 60.00, poderia manter os efeitos positivos com menor custo."
      ],
      "metadata": {
        "id": "__kH4U0PGjV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c) Recomenda√ß√µes de oportunidades de melhoria nessa a√ß√£o e nova proposta de teste A/B para validar essas hip√≥te-ses."
      ],
      "metadata": {
        "id": "79GNkhelLuJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìä **Resumo da campanha atual**\n",
        "\n",
        "A campanha com cupom aumentou o engajamento:\n",
        "\n",
        "* Reten√ß√£o subiu para **79,5%** (vs 74,7%)\n",
        "* Mais **pedidos por usu√°rio** (4,79 vs 4,23)\n",
        "* Mais **heavy users** (25,8% vs 21,5%)\n",
        "\n",
        "‚ùå **Problema**: o custo foi alto demais\n",
        "üí∏ **Resultado l√≠quido**: ‚ÄìR\\$ 19 milh√µes\n",
        "üìâ **ROI**: ‚Äì89.11%\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ **Oportunidades de melhoria**\n",
        "\n",
        "1. **Focar em usu√°rios inativos ou novos**, n√£o em todos.\n",
        "2. **Reduzir o valor do cupom** ou exigir **valor m√≠nimo de compra**.\n",
        "3. **Estimular o aumento do ticket m√©dio** com frete gr√°tis.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ **Nova proposta de teste A/B**\n",
        "\n",
        "| Grupo    | Estrat√©gia                          |\n",
        "| -------- | ----------------------------------- |\n",
        "| Controle | Sem cupom                           |\n",
        "| Teste A  | R\\$10 s√≥ para usu√°rios inativos  |\n",
        "| Teste B  | R\\$5 de desconto em pedidos > R\\$40     |\n",
        "| Teste C  | Frete Gr√°tis para pedidos acima de R$ 65,00         |\n",
        "\n",
        "**M√©tricas para analisar**: Receita incremental, Resultado L√≠quido, ROI, Reten√ß√£o, Reativa√ß√£o, Ticket m√©dio.\n"
      ],
      "metadata": {
        "id": "qQQhhWWbNMx1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCqMktILMA83"
      },
      "source": [
        "## O Desafio ‚Äì An√°lise por Segmenta√ß√£o (ETAPA 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a) Definindo as segmenta√ß√µes:"
      ],
      "metadata": {
        "id": "3RmkSrWSO8qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ab_orders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KNzhwpDUqTr",
        "outputId": "157a87a2-11f8-4a98-cbc5-82f07ef5c2df"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[customer_id: string, cpf: string, customer_name: string, delivery_address_city: string, delivery_address_country: string, delivery_address_district: string, delivery_address_external_id: string, delivery_address_latitude: string, delivery_address_longitude: string, delivery_address_state: string, delivery_address_zip_code: string, items: string, merchant_id: string, merchant_latitude: string, merchant_longitude: string, merchant_timezone: string, order_created_at: string, order_id: string, order_scheduled: boolean, order_scheduled_date: string, order_total_amount: float, origin_platform: string, is_target: string]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Cria ou reinicia a sess√£o Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SegmentacaoUsuarios\") \\\n",
        "    .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "wRD8aaQrVvcz"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, count, sum, avg, when, round\n",
        "\n",
        "# 1. Calcular m√©trica por usu√°rio\n",
        "df_user_metrics = df_ab_orders.groupBy(\"customer_id\", \"is_target\").agg(\n",
        "    count(\"order_id\").alias(\"qtd_pedidos\"),\n",
        "    round(sum(\"order_total_amount\") / count(\"order_id\"), 2).alias(\"ticket_medio\")\n",
        ")\n",
        "\n",
        "# 2. Calcular tercis\n",
        "qtd_tercis = df_user_metrics.approxQuantile(\"qtd_pedidos\", [0.33, 0.66], 0.01)\n",
        "ticket_tercis = df_user_metrics.approxQuantile(\"ticket_medio\", [0.33, 0.66], 0.01)\n",
        "\n",
        "# 3. Segmentar por frequ√™ncia\n",
        "df_user_metrics = df_user_metrics.withColumn(\n",
        "    \"segmento_frequencia\",\n",
        "    when(col(\"qtd_pedidos\") <= qtd_tercis[0], \"Baixa frequ√™ncia\")\n",
        "    .when(col(\"qtd_pedidos\") <= qtd_tercis[1], \"M√©dia frequ√™ncia\")\n",
        "    .otherwise(\"Alta frequ√™ncia\")\n",
        ")\n",
        "\n",
        "# 4. Segmentar por ticket m√©dio\n",
        "df_user_metrics = df_user_metrics.withColumn(\n",
        "    \"segmento_ticket\",\n",
        "    when(col(\"ticket_medio\") <= ticket_tercis[0], \"Ticket baixo\")\n",
        "    .when(col(\"ticket_medio\") <= ticket_tercis[1], \"Ticket m√©dio\")\n",
        "    .otherwise(\"Ticket alto\")\n",
        ")\n",
        "\n",
        "# 5. Contar usu√°rios por grupo e segmentos\n",
        "df_segmentos_final = df_user_metrics.groupBy(\"is_target\", \"segmento_frequencia\", \"segmento_ticket\").agg(\n",
        "    count(\"customer_id\").alias(\"qtd_usuarios\")\n",
        ").orderBy(\"is_target\", \"segmento_frequencia\", \"segmento_ticket\")\n",
        "\n",
        "df_segmentos_final.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ikIMDE5TSQUw",
        "outputId": "2864a012-8259-4a39-edbf-d0ce40526520"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o2440685.approxQuantile.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:61)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:58)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:169)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:168)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:588)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:538)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:277)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:417)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:402)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3849)\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3847)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.multipleApproxQuantiles(StatFunctions.scala:100)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:104)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:115)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-98-3665875361.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 2. Calcular tercis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mqtd_tercis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_user_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"qtd_pedidos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.66\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mticket_tercis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_user_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ticket_medio\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.66\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mapproxQuantile\u001b[0;34m(self, col, probabilities, relativeError)\u001b[0m\n\u001b[1;32m   4843\u001b[0m         \u001b[0mrelativeError\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelativeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4845\u001b[0;31m         \u001b[0mjaq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelativeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4846\u001b[0m         \u001b[0mjaq_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjaq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjaq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misStr\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mjaq_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2440685.approxQuantile.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:61)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:58)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:169)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:168)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:588)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:538)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:277)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:417)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:402)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3849)\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3847)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.multipleApproxQuantiles(StatFunctions.scala:100)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:104)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:115)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, count, avg\n",
        "\n",
        "# 1 - Agrupar usu√°rios com seus KPIs individuais\n",
        "df_user_stats = df_ab_orders.groupBy(\"customer_id\", \"is_target\").agg(\n",
        "    count(\"order_id\").alias(\"qtd_pedidos\"),\n",
        "    avg(\"order_total_amount\").alias(\"ticket_medio_usuario\")\n",
        ")\n",
        "\n",
        "# 2 - Criar segmentos por frequ√™ncia\n",
        "df_user_segmentado = df_user_stats.withColumn(\n",
        "    \"segmento_frequencia\",\n",
        "    when(col(\"qtd_pedidos\") <= 2, \"Baixa frequ√™ncia\")\n",
        "    .when((col(\"qtd_pedidos\") >= 3) & (col(\"qtd_pedidos\") <= 5), \"M√©dia frequ√™ncia\")\n",
        "    .otherwise(\"Alta frequ√™ncia\")\n",
        ")\n",
        "\n",
        "# 3 - Criar segmentos por ticket m√©dio\n",
        "df_user_segmentado = df_user_segmentado.withColumn(\n",
        "    \"segmento_ticket\",\n",
        "    when(col(\"ticket_medio_usuario\") <= 35, \"Ticket baixo\")\n",
        "    .when((col(\"ticket_medio_usuario\") > 35) & (col(\"ticket_medio_usuario\") <= 60), \"Ticket m√©dio\")\n",
        "    .otherwise(\"Ticket alto\")\n",
        ")\n",
        "\n",
        "# 4 - Quantidade de usu√°rios por grupo e segmento\n",
        "df_resultado_segmentado = df_user_segmentado.groupBy(\"is_target\", \"segmento_frequencia\", \"segmento_ticket\").agg(\n",
        "    count(\"customer_id\").alias(\"qtd_usuarios\")\n",
        ").orderBy(\"is_target\", \"segmento_frequencia\", \"segmento_ticket\")\n",
        "\n",
        "# Mostrar resultado\n",
        "df_resultado_segmentado.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-STl4sbO8W9",
        "outputId": "309dd259-3e1e-4958-8316-b6c5f87873fe"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------+---------------+------------+\n",
            "|is_target|segmento_frequencia|segmento_ticket|qtd_usuarios|\n",
            "+---------+-------------------+---------------+------------+\n",
            "|control  |Alta frequ√™ncia    |Ticket alto    |16599       |\n",
            "|control  |Alta frequ√™ncia    |Ticket baixo   |25757       |\n",
            "|control  |Alta frequ√™ncia    |Ticket m√©dio   |35265       |\n",
            "|control  |Baixa frequ√™ncia   |Ticket alto    |43860       |\n",
            "|control  |Baixa frequ√™ncia   |Ticket baixo   |84266       |\n",
            "|control  |Baixa frequ√™ncia   |Ticket m√©dio   |78443       |\n",
            "|control  |M√©dia frequ√™ncia   |Ticket alto    |16410       |\n",
            "|control  |M√©dia frequ√™ncia   |Ticket baixo   |26954       |\n",
            "|control  |M√©dia frequ√™ncia   |Ticket m√©dio   |32988       |\n",
            "|target   |Alta frequ√™ncia    |Ticket alto    |24703       |\n",
            "|target   |Alta frequ√™ncia    |Ticket baixo   |38549       |\n",
            "|target   |Alta frequ√™ncia    |Ticket m√©dio   |52090       |\n",
            "|target   |Baixa frequ√™ncia   |Ticket alto    |45679       |\n",
            "|target   |Baixa frequ√™ncia   |Ticket baixo   |87514       |\n",
            "|target   |Baixa frequ√™ncia   |Ticket m√©dio   |82287       |\n",
            "|target   |M√©dia frequ√™ncia   |Ticket alto    |25117       |\n",
            "|target   |M√©dia frequ√™ncia   |Ticket baixo   |40898       |\n",
            "|target   |M√©dia frequ√™ncia   |Ticket m√©dio   |49087       |\n",
            "+---------+-------------------+---------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, sum, avg, when, percent_rank, round as spark_round\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. SETUP E SIMULA√á√ÉO DE DADOS (SUBSTITUA PELO SEU DATAFRAME)\n",
        "# ==============================================================================\n",
        "spark = SparkSession.builder.appName(\"SegmentABTest_NoDate\").getOrCreate()\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. CRIA√á√ÉO DA BASE DE AN√ÅLISE POR USU√ÅRIO (FEATURE STORE)\n",
        "# ==============================================================================\n",
        "# (Item b) - Parte 1: Agregamos os dados para ter uma vis√£o √∫nica por cliente.\n",
        "print(\"--- Passo 1: Criando a base de an√°lise por usu√°rio... ---\")\n",
        "df_user_features = df_ab_orders.groupBy(\"customer_id\", \"is_target\").agg(\n",
        "    count(\"order_id\").alias(\"total_pedidos\"),\n",
        "    avg(\"order_total_amount\").alias(\"ticket_medio_usuario\"),\n",
        "    sum(\"order_total_amount\").alias(\"gasto_total_usuario\")\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. APLICA√á√ÉO DOS CRIT√âRIOS DE SEGMENTA√á√ÉO\n",
        "# ==============================================================================\n",
        "# (Item a e b) - Parte 2: Adicionamos as \"etiquetas\" de segmento a cada usu√°rio.\n",
        "print(\"--- Passo 2: Aplicando os crit√©rios de segmenta√ß√£o... ---\")\n",
        "\n",
        "# --- Segmenta√ß√£o por Frequ√™ncia ---\n",
        "window_freq = Window.partitionBy(\"is_target\").orderBy(\"total_pedidos\")\n",
        "df_user_features = df_user_features.withColumn(\"rank_freq\", percent_rank().over(window_freq))\n",
        "df_user_features = df_user_features.withColumn(\"segmento_frequencia\",\n",
        "    when(col(\"rank_freq\") >= 0.8, \"1. Heavy User (Top 20%)\")\n",
        "    .when((col(\"rank_freq\") >= 0.3) & (col(\"rank_freq\") < 0.8), \"2. Casual (30-80%)\")\n",
        "    .otherwise(\"3. Leve (Bottom 30%)\")\n",
        ")\n",
        "\n",
        "# --- Segmenta√ß√£o por Valor (Ticket M√©dio) ---\n",
        "window_valor = Window.partitionBy(\"is_target\").orderBy(\"ticket_medio_usuario\")\n",
        "df_user_features = df_user_features.withColumn(\"rank_valor\", percent_rank().over(window_valor))\n",
        "df_user_features = df_user_features.withColumn(\"segmento_valor\",\n",
        "    when(col(\"rank_valor\") >= 0.5, \"1. Alto Valor\")\n",
        "    .otherwise(\"2. Baixo Valor\")\n",
        ")\n",
        "\n",
        "print(\"\\n--- DataFrame de Usu√°rios com Segmentos (Amostra) ---\")\n",
        "df_user_features.select(\n",
        "    \"customer_id\", \"is_target\", \"total_pedidos\", \"ticket_medio_usuario\",\n",
        "    \"segmento_frequencia\", \"segmento_valor\"\n",
        ").show(10, truncate=False)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. AN√ÅLISE CRUZADA FINAL: KPIs POR SEGMENTO\n",
        "# ==============================================================================\n",
        "# (Item c) - Agrupamos por grupo de teste E por segmento para gerar os relat√≥rios.\n",
        "print(\"--- Passo 3: Gerando as an√°lises cruzadas por segmento... ---\")\n",
        "\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*70)\n",
        "print(\"          (c) AN√ÅLISE DE RESULTADOS POR SEGMENTO DE FREQU√äNCIA\")\n",
        "print(\"=\"*70)\n",
        "df_analise_frequencia = df_user_features.groupBy(\"is_target\", \"segmento_frequencia\").agg(\n",
        "    count(\"customer_id\").alias(\"qtd_usuarios\"),\n",
        "    spark_round(avg(\"total_pedidos\"), 2).alias(\"pedidos_por_usuario\"),\n",
        "    spark_round(avg(\"ticket_medio_usuario\"), 2).alias(\"ticket_medio_segmento\")\n",
        ").orderBy(\"segmento_frequencia\", \"is_target\")\n",
        "df_analise_frequencia.show(truncate=False)\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"          (c) AN√ÅLISE DE RESULTADOS POR SEGMENTO DE VALOR (TICKET M√âDIO)\")\n",
        "print(\"=\"*70)\n",
        "df_analise_valor = df_user_features.groupBy(\"is_target\", \"segmento_valor\").agg(\n",
        "    count(\"customer_id\").alias(\"qtd_usuarios\"),\n",
        "    spark_round(avg(\"total_pedidos\"), 2).alias(\"pedidos_por_usuario\"),\n",
        "    spark_round(avg(\"ticket_medio_usuario\"), 2).alias(\"ticket_medio_segmento\")\n",
        ").orderBy(\"segmento_valor\", \"is_target\")\n",
        "df_analise_valor.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N-yv3OLuPXB9",
        "outputId": "8d39f9b9-bf65-497c-b203-b3d6f35a2b5b"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Passo 1: Criando a base de an√°lise por usu√°rio... ---\n",
            "--- Passo 2: Aplicando os crit√©rios de segmenta√ß√£o... ---\n",
            "\n",
            "--- DataFrame de Usu√°rios com Segmentos (Amostra) ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o2455895.showString.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:61)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:58)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:169)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:168)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:588)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:538)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:534)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:277)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:417)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-101-282026614.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;34m\"customer_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"is_target\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"total_pedidos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ticket_medio_usuario\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;34m\"segmento_frequencia\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"segmento_valor\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m ).show(10, truncate=False)\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    974\u001b[0m                 )\n\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_truncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2455895.showString.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:61)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:58)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:169)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:168)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:588)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:538)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:534)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:277)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:417)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZiwHRYuMdyl"
      },
      "source": [
        "## O Desafio ‚Äì Pr√≥ximos Passos Ifood (ETAPA 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwA-iRJZMwra"
      },
      "source": [
        "## Gr√°ficos para o relat√≥rio final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WtnFnYJGpcn"
      },
      "source": [
        "## Testes (Apagar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTaELEpyGsYA",
        "outputId": "7c41a6d1-0c20-4164-cc68-a18cd9054b9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "|order_date|\n",
            "+----------+\n",
            "|2018-12-03|\n",
            "|2018-12-04|\n",
            "|2018-12-05|\n",
            "|2018-12-06|\n",
            "|2018-12-07|\n",
            "|2018-12-08|\n",
            "|2018-12-09|\n",
            "|2018-12-10|\n",
            "|2018-12-11|\n",
            "|2018-12-12|\n",
            "|2018-12-13|\n",
            "|2018-12-14|\n",
            "|2018-12-15|\n",
            "|2018-12-16|\n",
            "|2018-12-17|\n",
            "|2018-12-18|\n",
            "|2018-12-19|\n",
            "|2018-12-20|\n",
            "|2018-12-21|\n",
            "|2018-12-22|\n",
            "+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, to_date\n",
        "\n",
        "df_order_distinct = df_order.select(to_date(col(\"order_created_at\")).alias(\"order_date\")).distinct().orderBy(\"order_date\")\n",
        "df_order_distinct.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV8g1irCIZtV",
        "outputId": "04b8e71d-b466-4302-924c-6f2414171941"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_order_distinct.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uojtMbKJCoC",
        "outputId": "a545e7b9-e43c-4786-ca1e-71be8ab85f7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+------------+-----------+-------------+\n",
            "|is_target|qtd_usuarios|qtd_pedidos|taxa_ativacao|\n",
            "+---------+------------+-----------+-------------+\n",
            "|  control|      360542|    1525576|       423.13|\n",
            "|   target|      445924|    2136745|       479.17|\n",
            "+---------+------------+-----------+-------------+\n",
            "\n",
            "+---------+--------------+----------------+-------------+\n",
            "|is_target|total_usuarios|usuarios_retidos|taxa_retencao|\n",
            "+---------+--------------+----------------+-------------+\n",
            "|  control|        360542|          269341|         74.7|\n",
            "|   target|        445924|          354538|        79.51|\n",
            "+---------+--------------+----------------+-------------+\n",
            "\n",
            "+-------+------+---------------------+\n",
            "|control|target|incremento_percentual|\n",
            "+-------+------+---------------------+\n",
            "|   47.9| 47.74|                -0.33|\n",
            "+-------+------+---------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, countDistinct, count, when, round, avg, first\n",
        "\n",
        "# 1 - Calculando taxa de ativa√ß√£o (% usu√°rios que fizeram ao menos 1 pedido)\n",
        "df_ativacao = df_ab_orders.groupBy(\"is_target\").agg(\n",
        "    countDistinct(\"customer_id\").alias(\"qtd_usuarios\"),\n",
        "    count(\"order_id\").alias(\"qtd_pedidos\")\n",
        ").withColumn(\n",
        "    \"taxa_ativacao\", round((col(\"qtd_pedidos\") / col(\"qtd_usuarios\")) * 100, 2)\n",
        ")\n",
        "\n",
        "# 2 - Calculando taxa de reten√ß√£o (% usu√°rios com mais de 1 pedido)\n",
        "df_pedidos_usuario = df_ab_orders.groupBy(\"is_target\", \"customer_id\").agg(\n",
        "    count(\"order_id\").alias(\"qtd_pedidos\")\n",
        ")\n",
        "\n",
        "df_retencao = df_pedidos_usuario.groupBy(\"is_target\").agg(\n",
        "    count(\"customer_id\").alias(\"total_usuarios\"),\n",
        "    count(when(col(\"qtd_pedidos\") > 1, True)).alias(\"usuarios_retidos\")\n",
        ").withColumn(\n",
        "    \"taxa_retencao\", round((col(\"usuarios_retidos\") / col(\"total_usuarios\")) * 100, 2)\n",
        ")\n",
        "\n",
        "# 3 - Calculando incremento percentual no ticket m√©dio\n",
        "df_ticket_medio = df_ab_orders.groupBy(\"is_target\").agg(\n",
        "    round(avg(\"order_total_amount\"), 2).alias(\"ticket_medio\")\n",
        ")\n",
        "\n",
        "df_incremento = df_ticket_medio.groupBy().pivot(\"is_target\").agg(first(\"ticket_medio\")).withColumn(\n",
        "    \"incremento_percentual\", round(((col(\"target\") - col(\"control\")) / col(\"control\")) * 100, 2)\n",
        ")\n",
        "\n",
        "# Exibindo os resultados\n",
        "df_ativacao.show()\n",
        "df_retencao.show()\n",
        "df_incremento.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StwPTlAjOX0u",
        "outputId": "6bbdf12f-a2e5-4fc2-cc40-30ea06dc21ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "qtd_usuarios qtd_pedidos ticket_medio valor_total_pedidos pedidos_por_usuario taxa_retencao incremento_percentual\n",
            "     360.542   1.525.576     R$ 47,90    R$ 73.071.872,88                 4,2        74,70%                -0,33%\n",
            "     445.924   2.136.745     R$ 47,74   R$ 102.007.569,84                 4,8        79,51%                   NaN\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, countDistinct, count, sum, avg, when, round, first\n",
        "import pandas as pd\n",
        "\n",
        "# 1 - Juntando pedidos com grupo de teste\n",
        "df_ab_orders = df_order.join(df_ab_test, on=\"customer_id\", how=\"inner\")\n",
        "\n",
        "# 2 - Calculando m√©tricas principais\n",
        "df_metrics = df_ab_orders.groupBy(\"is_target\").agg(\n",
        "    countDistinct(\"customer_id\").alias(\"qtd_usuarios\"),\n",
        "    count(\"order_id\").alias(\"qtd_pedidos\"),\n",
        "    avg(\"order_total_amount\").alias(\"ticket_medio\"),\n",
        "    sum(\"order_total_amount\").alias(\"valor_total_pedidos\")\n",
        ").withColumn(\n",
        "    \"pedidos_por_usuario\",\n",
        "    col(\"qtd_pedidos\") / col(\"qtd_usuarios\")\n",
        ")\n",
        "\n",
        "# 3 - Taxa de Reten√ß√£o\n",
        "df_pedidos_usuario = df_ab_orders.groupBy(\"is_target\", \"customer_id\").agg(\n",
        "    count(\"order_id\").alias(\"qtd_pedidos\")\n",
        ")\n",
        "\n",
        "df_retencao = df_pedidos_usuario.groupBy(\"is_target\").agg(\n",
        "    count(\"customer_id\").alias(\"total_usuarios\"),\n",
        "    count(when(col(\"qtd_pedidos\") > 1, True)).alias(\"usuarios_retidos\")\n",
        ").withColumn(\n",
        "    \"taxa_retencao\", round((col(\"usuarios_retidos\") / col(\"total_usuarios\")) * 100, 2)\n",
        ")\n",
        "\n",
        "# 4 - Incremento Percentual no Ticket M√©dio\n",
        "df_ticket_medio = df_ab_orders.groupBy(\"is_target\").agg(\n",
        "    round(avg(\"order_total_amount\"), 2).alias(\"ticket_medio\")\n",
        ")\n",
        "\n",
        "df_incremento = df_ticket_medio.groupBy().pivot(\"is_target\").agg(first(\"ticket_medio\")).withColumn(\n",
        "    \"incremento_percentual\", round(((col(\"target\") - col(\"control\")) / col(\"control\")) * 100, 2)\n",
        ")\n",
        "\n",
        "# 5 - Convertendo para Pandas\n",
        "df_metrics_pd = df_metrics.toPandas()\n",
        "df_retencao_pd = df_retencao.select(\"is_target\", \"taxa_retencao\").toPandas()\n",
        "df_incremento_pd = df_incremento.select(\"incremento_percentual\").toPandas()\n",
        "\n",
        "# 6 - Mesclando tudo\n",
        "df_final = df_metrics_pd.merge(df_retencao_pd, on=\"is_target\", how=\"left\")\n",
        "\n",
        "# 7 - Formatando os valores\n",
        "df_formatado = pd.DataFrame()\n",
        "#df_formatado[\"Grupo\"] = df_final[\"is_target\"]\n",
        "df_formatado[\"qtd_usuarios\"] = df_final[\"qtd_usuarios\"].apply(lambda x: f\"{x:,.0f}\".replace(\",\", \".\"))\n",
        "df_formatado[\"qtd_pedidos\"] = df_final[\"qtd_pedidos\"].apply(lambda x: f\"{x:,.0f}\".replace(\",\", \".\"))\n",
        "df_formatado[\"ticket_medio\"] = df_final[\"ticket_medio\"].apply(lambda x: f\"R$ {x:,.2f}\".replace(\",\", \"X\").replace(\".\", \",\").replace(\"X\", \".\"))\n",
        "df_formatado[\"valor_total_pedidos\"] = df_final[\"valor_total_pedidos\"].apply(lambda x: f\"R$ {x:,.2f}\".replace(\",\", \"X\").replace(\".\", \",\").replace(\"X\", \".\"))\n",
        "df_formatado[\"pedidos_por_usuario\"] = df_final[\"pedidos_por_usuario\"].apply(lambda x: f\"{x:.1f}\".replace(\".\", \",\"))\n",
        "df_formatado[\"taxa_retencao\"] = df_final[\"taxa_retencao\"].apply(lambda x: f\"{x:.2f}%\".replace(\".\", \",\"))\n",
        "df_formatado[\"incremento_percentual\"] = df_incremento_pd[\"incremento_percentual\"].apply(lambda x: f\"{x:.2f}%\".replace(\".\", \",\"))\n",
        "\n",
        "# 8 - Exibindo resultado\n",
        "print(df_formatado.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nquj3lFpRHNn",
        "outputId": "df35c6d7-1845-4c8e-b4b9-6b95bf4df430"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "PASSO 1: Agregando para contar pedidos por cliente...\n",
            "+--------------------+-----------+\n",
            "|         customer_id|order_count|\n",
            "+--------------------+-----------+\n",
            "|0001226e517517758...|          4|\n",
            "|00021cd56b6d6c980...|          5|\n",
            "|00021f6dc15d10418...|          3|\n",
            "|00022b8c0c7af061f...|          6|\n",
            "|00029b26fb2121119...|          1|\n",
            "+--------------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, avg\n",
        "\n",
        "# PASSO 1: Contar o n√∫mero de pedidos por cliente\n",
        "# Agrupamos pelo identificador do cliente e contamos quantos pedidos ele tem.\n",
        "print(\"\\nPASSO 1: Agregando para contar pedidos por cliente...\")\n",
        "df_user_counts = df_ab_orders.groupBy(\"customer_id\").agg(\n",
        "    count(\"order_id\").alias(\"order_count\")\n",
        ")\n",
        "df_user_counts.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-IXZ4_gRgbo",
        "outputId": "fc3acc0e-bfa0-4989-daf0-57fe0918f0e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "PASSO 2: Calculando os percentis sobre a contagem de pedidos...\n",
            "\n",
            "O limite para estar no TOP 20% (percentil 80) √©: 6.0 pedidos.\n",
            "O limite para estar no TOP 10% (percentil 90) √©: 10.0 pedidos.\n",
            "O limite para estar no TOP 5% (percentil 95) √©: 14.0 pedidos.\n"
          ]
        }
      ],
      "source": [
        "# PASSO 2: Calcular os percentis para definir o que √© um \"Heavy User\"\n",
        "# Usamos o DataFrame agregado do passo anterior para esta an√°lise.\n",
        "print(\"\\nPASSO 2: Calculando os percentis sobre a contagem de pedidos...\")\n",
        "percentiles_to_check = [0.80, 0.90, 0.95]  # Top 20%, 10% e 5%\n",
        "# O terceiro argumento (0.01) √© a margem de erro permitida, para performance. 0.0 para exatid√£o.\n",
        "quantiles = df_user_counts.approxQuantile(\"order_count\", percentiles_to_check, 0.01)\n",
        "\n",
        "p80_threshold = quantiles[0]\n",
        "p90_threshold = quantiles[1]\n",
        "p95_threshold = quantiles[2]\n",
        "\n",
        "print(f\"\\nO limite para estar no TOP 20% (percentil 80) √©: {p80_threshold} pedidos.\")\n",
        "print(f\"O limite para estar no TOP 10% (percentil 90) √©: {p90_threshold} pedidos.\")\n",
        "print(f\"O limite para estar no TOP 5% (percentil 95) √©: {p95_threshold} pedidos.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bySblv1LRm1A",
        "outputId": "3523b634-0f49-4649-ff15-e635c7e1a584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Definindo 'Heavy Users' como clientes com mais de 6.0 pedidos...\n",
            "\n",
            "--- RESULTADO FINAL ---\n",
            "M√©dia de pedidos para o grupo de Heavy Users:\n",
            "+-------------------------+\n",
            "|media_pedidos_heavy_users|\n",
            "+-------------------------+\n",
            "|       13.258736224903803|\n",
            "+-------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# PASSO 3: Escolher um limite, filtrar os Heavy Users e calcular a nova m√©dia\n",
        "# Vamos usar o Top 20% (percentil 80) como nosso crit√©rio para Heavy User.\n",
        "# Sinta-se √† vontade para trocar p80_threshold por p90_threshold ou outro valor.\n",
        "heavy_user_threshold = p80_threshold\n",
        "\n",
        "print(f\"\\nDefinindo 'Heavy Users' como clientes com mais de {heavy_user_threshold} pedidos...\")\n",
        "\n",
        "df_heavy_users = df_user_counts.filter(col(\"order_count\") > heavy_user_threshold)\n",
        "\n",
        "# Finalmente, calculamos a m√©dia de pedidos apenas para este grupo filtrado.\n",
        "heavy_user_avg_df = df_heavy_users.agg(\n",
        "    avg(\"order_count\").alias(\"media_pedidos_heavy_users\")\n",
        ")\n",
        "\n",
        "\n",
        "print(\"\\n--- RESULTADO FINAL ---\")\n",
        "print(\"M√©dia de pedidos para o grupo de Heavy Users:\")\n",
        "heavy_user_avg_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtP6nAGvSyHc",
        "outputId": "1536506e-eb31-4a97-b648-887b41ba9f7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîù Percentil 80 (limiar de pedidos): 6.0\n",
            "is_target qtd_heavy_users total_usuarios percentual_heavy_users\n",
            "  control          77.621        360.542                 21,53%\n",
            "   target         115.342        445.924                 25,87%\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, count, when, lit\n",
        "\n",
        "# 1 - Calculando a quantidade de pedidos por usu√°rio\n",
        "df_pedidos_usuario = df_ab_orders.groupBy(\"customer_id\").agg(\n",
        "    count(\"order_id\").alias(\"qtd_pedidos\")\n",
        ")\n",
        "\n",
        "# 2 - Calculando o valor do percentil 80 (limiar dos heavy users)\n",
        "percentil_80 = df_pedidos_usuario.approxQuantile(\"qtd_pedidos\", [0.80], 0.01)[0]\n",
        "\n",
        "# 3 - Classificando os usu√°rios como heavy users (acima ou igual ao percentil 80)\n",
        "df_pedidos_usuario = df_pedidos_usuario.withColumn(\n",
        "    \"heavy_user\", when(col(\"qtd_pedidos\") >= percentil_80, lit(1)).otherwise(lit(0))\n",
        ")\n",
        "\n",
        "# 4 - Juntando com a classifica√ß√£o A/B\n",
        "df_heavy = df_pedidos_usuario.join(df_ab_test, on=\"customer_id\", how=\"inner\")\n",
        "\n",
        "# 5 - Contando heavy users por grupo (target e control)\n",
        "df_heavy_count = df_heavy.groupBy(\"is_target\").agg(\n",
        "    count(when(col(\"heavy_user\") == 1, True)).alias(\"qtd_heavy_users\"),\n",
        "    count(\"*\").alias(\"total_usuarios\")\n",
        ").withColumn(\n",
        "    \"percentual_heavy_users\", round((col(\"qtd_heavy_users\") / col(\"total_usuarios\")) * 100, 2)\n",
        ")\n",
        "\n",
        "# 6 - Convertendo para Pandas\n",
        "df_heavy_pd = df_heavy_count.toPandas()\n",
        "\n",
        "# 7 - Formatando resultados\n",
        "df_heavy_pd[\"qtd_heavy_users\"] = df_heavy_pd[\"qtd_heavy_users\"].apply(lambda x: f\"{x:,}\".replace(\",\", \".\"))\n",
        "df_heavy_pd[\"total_usuarios\"] = df_heavy_pd[\"total_usuarios\"].apply(lambda x: f\"{x:,}\".replace(\",\", \".\"))\n",
        "df_heavy_pd[\"percentual_heavy_users\"] = df_heavy_pd[\"percentual_heavy_users\"].apply(lambda x: f\"{x:.2f}%\".replace(\".\", \",\"))\n",
        "\n",
        "# 8 - Exibindo\n",
        "print(\"üîù Percentil 80 (limiar de pedidos):\", percentil_80)\n",
        "print(df_heavy_pd.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fmnCc4z4VrBv",
        "outputId": "97fd32b1-c018-4a52-f315-5eac623eb287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä M√©dia de pedidos dos heavy users: 11.75\n",
            "üîù Limite para estar no TOP 20% (percentil 80): 6.0 pedidos\n",
            "is_target qtd_heavy_users total_usuarios percentual_heavy_users\n",
            "  control          77.621        360.542                 21,53%\n",
            "   target         115.342        445.924                 25,87%\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, count, when, lit, avg, round\n",
        "\n",
        "# 1 - Calculando a quantidade de pedidos por usu√°rio\n",
        "df_pedidos_usuario = df_ab_orders.groupBy(\"customer_id\").agg(\n",
        "    count(\"order_id\").alias(\"qtd_pedidos\")\n",
        ")\n",
        "\n",
        "# 2 - Calculando o valor do percentil 80 (limiar dos heavy users)\n",
        "percentil_80 = df_pedidos_usuario.approxQuantile(\"qtd_pedidos\", [0.80], 0.01)[0]\n",
        "\n",
        "# 3 - Classificando os usu√°rios como heavy users (acima ou igual ao percentil 80)\n",
        "df_pedidos_usuario = df_pedidos_usuario.withColumn(\n",
        "    \"heavy_user\", when(col(\"qtd_pedidos\") >= percentil_80, lit(1)).otherwise(lit(0))\n",
        ")\n",
        "\n",
        "# 4 - Juntando com a classifica√ß√£o A/B\n",
        "df_heavy = df_pedidos_usuario.join(df_ab_test, on=\"customer_id\", how=\"inner\")\n",
        "\n",
        "# 5 - Contando heavy users por grupo (target e control)\n",
        "df_heavy_count = df_heavy.groupBy(\"is_target\").agg(\n",
        "    count(when(col(\"heavy_user\") == 1, True)).alias(\"qtd_heavy_users\"),\n",
        "    count(\"*\").alias(\"total_usuarios\")\n",
        ").withColumn(\n",
        "    \"percentual_heavy_users\", round((col(\"qtd_heavy_users\") / col(\"total_usuarios\")) * 100, 2)\n",
        ")\n",
        "\n",
        "# 6 - Calculando m√©dia de pedidos dos heavy users\n",
        "media_pedidos_heavy_users = df_heavy.filter(col(\"heavy_user\") == 1).agg(\n",
        "    round(avg(\"qtd_pedidos\"), 2).alias(\"media_pedidos_heavy_users\")\n",
        ").collect()[0][\"media_pedidos_heavy_users\"]\n",
        "\n",
        "# 7 - Convertendo para Pandas\n",
        "df_heavy_pd = df_heavy_count.toPandas()\n",
        "\n",
        "# 8 - Formatando resultados\n",
        "df_heavy_pd[\"qtd_heavy_users\"] = df_heavy_pd[\"qtd_heavy_users\"].apply(lambda x: f\"{x:,}\".replace(\",\", \".\"))\n",
        "df_heavy_pd[\"total_usuarios\"] = df_heavy_pd[\"total_usuarios\"].apply(lambda x: f\"{x:,}\".replace(\",\", \".\"))\n",
        "df_heavy_pd[\"percentual_heavy_users\"] = df_heavy_pd[\"percentual_heavy_users\"].apply(lambda x: f\"{x:.2f}%\".replace(\".\", \",\"))\n",
        "\n",
        "# 9 - Exibindo\n",
        "print(f\"üìä M√©dia de pedidos dos heavy users: {media_pedidos_heavy_users}\")\n",
        "print(f\"üîù Limite para estar no TOP 20% (percentil 80): {percentil_80} pedidos\")\n",
        "print(df_heavy_pd.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZoN3JyiiC2e",
        "outputId": "33d5f001-fc0c-424b-ac2f-30cdb1cb28fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-------+-----------+--------------+------------+-------------+-------------------+-----------------+--------------+--------------+----------------+\n",
            "|                  id|          created_at|enabled|price_range|average_ticket|takeout_time|delivery_time|minimum_order_value|merchant_zip_code| merchant_city|merchant_state|merchant_country|\n",
            "+--------------------+--------------------+-------+-----------+--------------+------------+-------------+-------------------+-----------------+--------------+--------------+----------------+\n",
            "|d19ff6fca6288939b...|2017-01-23 12:52:...|  false|          3|          60.0|           0|           50|               30.0|            14025|RIBEIRAO PRETO|            SP|              BR|\n",
            "|631df0985fdbbaf27...|2017-01-20 13:14:...|   true|          3|          60.0|           0|            0|               30.0|            50180|     SAO PAULO|            SP|              BR|\n",
            "|135c5c4ae4c1ec1fd...|2017-01-23 12:46:...|   true|          5|         100.0|           0|           45|               10.0|            23090|RIO DE JANEIRO|            RJ|              BR|\n",
            "+--------------------+--------------------+-------+-----------+--------------+------------+-------------+-------------------+-----------------+--------------+--------------+----------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_restaurant.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYyGSWyEiAmb",
        "outputId": "3768c549-def6-4f8b-c27a-9870726df382"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîù N√∫mero de restaurantes que fazem 80% dos pedidos: 2629\n",
            "+---------+------------+-----------+--------------------+------------------+-------------------+\n",
            "|is_target|qtd_usuarios|qtd_pedidos| valor_total_pedidos|      ticket_medio|pedidos_por_usuario|\n",
            "+---------+------------+-----------+--------------------+------------------+-------------------+\n",
            "|  control|      360542|    1525576| 7.307187305451663E7|47.897891061813134|  4.231340592774212|\n",
            "|   target|      445924|    2136745|1.0200757006332143E8| 47.73970224023991|   4.79172459881056|\n",
            "+---------+------------+-----------+--------------------+------------------+-------------------+\n",
            "\n",
            "+---------+--------------+----------------+-------------+\n",
            "|is_target|total_usuarios|usuarios_retidos|taxa_retencao|\n",
            "+---------+--------------+----------------+-------------+\n",
            "|  control|        360542|          269341|         74.7|\n",
            "|   target|        445924|          354538|        79.51|\n",
            "+---------+--------------+----------------+-------------+\n",
            "\n",
            "+-------+------+---------------------+\n",
            "|control|target|incremento_percentual|\n",
            "+-------+------+---------------------+\n",
            "|   47.9| 47.74|                -0.33|\n",
            "+-------+------+---------------------+\n",
            "\n",
            "+---------+---------------+-----------+----------------------+\n",
            "|is_target|qtd_heavy_users|total_users|percentual_heavy_users|\n",
            "+---------+---------------+-----------+----------------------+\n",
            "|  control|          77621|     360542|                 21.53|\n",
            "|   target|         115342|     445924|                 25.87|\n",
            "+---------+---------------+-----------+----------------------+\n",
            "\n",
            "+---------+-------------------+\n",
            "|is_target|tempo_medio_entrega|\n",
            "+---------+-------------------+\n",
            "|  control|              22.63|\n",
            "|   target|              22.66|\n",
            "+---------+-------------------+\n",
            "\n",
            "+---------+----------------+\n",
            "|is_target|qtd_restaurantes|\n",
            "+---------+----------------+\n",
            "|  control|            7196|\n",
            "|   target|            7227|\n",
            "+---------+----------------+\n",
            "\n",
            "+---------+-----------------------------+\n",
            "|is_target|media_pedidos_por_restaurante|\n",
            "+---------+-----------------------------+\n",
            "|  control|                        212.0|\n",
            "|   target|                       295.66|\n",
            "+---------+-----------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, countDistinct, count, sum, avg, when, round, first, lit\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder.appName(\"iFood_AB_KPIs\").getOrCreate()\n",
        "\n",
        "# Tipagem\n",
        "df_order = df_order.withColumn(\"order_total_amount\", col(\"order_total_amount\").cast(\"float\"))\n",
        "df_restaurant = df_restaurant.withColumn(\"delivery_time\", col(\"delivery_time\").cast(\"float\"))\n",
        "\n",
        "# Base principal\n",
        "df_ab_orders = df_order.join(df_ab_test, on=\"customer_id\", how=\"inner\")\n",
        "\n",
        "# KPIs principais\n",
        "df_metrics = df_ab_orders.groupBy(\"is_target\").agg(\n",
        "    countDistinct(\"customer_id\").alias(\"qtd_usuarios\"),\n",
        "    count(\"order_id\").alias(\"qtd_pedidos\"),\n",
        "    sum(\"order_total_amount\").alias(\"valor_total_pedidos\"),\n",
        "    avg(\"order_total_amount\").alias(\"ticket_medio\")\n",
        ").withColumn(\"pedidos_por_usuario\", col(\"qtd_pedidos\") / col(\"qtd_usuarios\"))\n",
        "\n",
        "# Reten√ß√£o\n",
        "df_retent = df_ab_orders.groupBy(\"is_target\", \"customer_id\").agg(count(\"order_id\").alias(\"qtd_pedidos\"))\n",
        "df_retencao = df_retent.groupBy(\"is_target\").agg(\n",
        "    count(\"customer_id\").alias(\"total_usuarios\"),\n",
        "    count(when(col(\"qtd_pedidos\") > 1, True)).alias(\"usuarios_retidos\")\n",
        ").withColumn(\"taxa_retencao\", round(col(\"usuarios_retidos\") / col(\"total_usuarios\") * 100, 2))\n",
        "\n",
        "# Incremento percentual\n",
        "df_ticket_medio = df_ab_orders.groupBy(\"is_target\").agg(round(avg(\"order_total_amount\"), 2).alias(\"ticket_medio\"))\n",
        "df_incremento = df_ticket_medio.groupBy().pivot(\"is_target\").agg(first(\"ticket_medio\")).withColumn(\n",
        "    \"incremento_percentual\", round((col(\"target\") - col(\"control\")) / col(\"control\") * 100, 2)\n",
        ")\n",
        "\n",
        "# Heavy users\n",
        "df_user_pedidos = df_ab_orders.groupBy(\"customer_id\", \"is_target\").agg(count(\"order_id\").alias(\"qtd_pedidos\"))\n",
        "percentil_80 = df_user_pedidos.approxQuantile(\"qtd_pedidos\", [0.80], 0.01)[0]\n",
        "df_heavy_users = df_user_pedidos.withColumn(\"heavy_user\", when(col(\"qtd_pedidos\") >= percentil_80, 1).otherwise(0))\n",
        "df_heavy_summary = df_heavy_users.groupBy(\"is_target\").agg(\n",
        "    count(when(col(\"heavy_user\") == 1, True)).alias(\"qtd_heavy_users\"),\n",
        "    count(\"*\").alias(\"total_users\")\n",
        ").withColumn(\"percentual_heavy_users\", round(col(\"qtd_heavy_users\") / col(\"total_users\") * 100, 2))\n",
        "\n",
        "# Join com restaurantes\n",
        "df_ab_restaurant = df_ab_orders.join(df_restaurant, df_ab_orders[\"merchant_id\"] == df_restaurant[\"id\"], \"inner\")\n",
        "\n",
        "# Tempo m√©dio de entrega\n",
        "df_delivery_time = df_ab_restaurant.groupBy(\"is_target\").agg(\n",
        "    round(avg(\"delivery_time\"), 2).alias(\"tempo_medio_entrega\")\n",
        ")\n",
        "\n",
        "# Restaurantes por grupo\n",
        "df_qtd_rest = df_ab_restaurant.groupBy(\"is_target\").agg(countDistinct(\"merchant_id\").alias(\"qtd_restaurantes\"))\n",
        "\n",
        "# M√©dia de pedidos por restaurante\n",
        "df_pedidos_por_rest = df_ab_restaurant.groupBy(\"is_target\", \"merchant_id\").agg(count(\"order_id\").alias(\"pedidos\"))\n",
        "df_avg_pedidos_rest = df_pedidos_por_rest.groupBy(\"is_target\").agg(\n",
        "    round(avg(\"pedidos\"), 2).alias(\"media_pedidos_por_restaurante\")\n",
        ")\n",
        "\n",
        "# Pareto 80/20 dos restaurantes\n",
        "df_total_pedidos = df_ab_restaurant.groupBy(\"merchant_id\").agg(count(\"*\").alias(\"total_pedidos\"))\n",
        "total_geral = df_total_pedidos.agg(sum(\"total_pedidos\")).first()[0]\n",
        "df_pareto = df_total_pedidos.orderBy(col(\"total_pedidos\").desc())\n",
        "df_pareto = df_pareto.withColumn(\"acumulado\", F.sum(\"total_pedidos\").over(Window.orderBy(col(\"total_pedidos\").desc())))\n",
        "df_pareto = df_pareto.withColumn(\"acumulado_perc\", col(\"acumulado\") / lit(total_geral))\n",
        "top_20_restaurantes = df_pareto.filter(col(\"acumulado_perc\") <= 0.8).count()\n",
        "\n",
        "# Exibir resultados principais\n",
        "print(\"üîù N√∫mero de restaurantes que fazem 80% dos pedidos:\", top_20_restaurantes)\n",
        "df_metrics.show()\n",
        "df_retencao.show()\n",
        "df_incremento.show()\n",
        "df_heavy_summary.show()\n",
        "df_delivery_time.show()\n",
        "df_qtd_rest.show()\n",
        "df_avg_pedidos_rest.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMQbGXoShTZKmosh/MZ4yHV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}